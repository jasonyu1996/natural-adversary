{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "from settings import EXPERIMENTS_DIR\n",
    "from experiment import Experiment\n",
    "from utils2 import to_device, load_weights, load_embeddings, create_embeddings_matrix\n",
    "from vocab import Vocab\n",
    "from train2 import create_model\n",
    "from preprocess import load_dataset, create_dataset_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from models import MLPClassifier, Baseline_Embeddings\n",
    "from models import Seq2Seq, MLP_D, MLP_G, MLP_I, MLP_I_AE, JSDistance, Seq2SeqCAE, Baseline_Embeddings, Baseline_LSTM\n",
    "from utils import to_gpu, Corpus, batchify, SNLIDataset, collate_snli\n",
    "import random\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = 'train.3ir9y_e3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/experiments')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPERIMENTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment.load(EXPERIMENTS_DIR, exp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 453655, val: 10000, test: 10000\n",
      "Vocab: 9419, style vocab: 2\n",
      "W_emb: (9419, 300)\n"
     ]
    }
   ],
   "source": [
    "preprocess_exp = Experiment.load(EXPERIMENTS_DIR, exp.config.preprocess_exp_id)\n",
    "dataset_train, dataset_val, dataset_test, vocab, style_vocab, W_emb = load_dataset(preprocess_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_reader = create_dataset_reader(preprocess_exp.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(exp.config, vocab, style_vocab, dataset_train.max_len, W_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights(model, exp.experiment_dir.joinpath('best.th'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs(instances):\n",
    "    if not isinstance(instances, list):\n",
    "        instances = [instances,]\n",
    "        \n",
    "    if not isinstance(instances[0], dict):\n",
    "        sentences = [\n",
    "            dataset_reader.preprocess_sentence(dataset_reader.spacy( dataset_reader.clean_sentence(sent)))\n",
    "            for sent in instances\n",
    "        ]\n",
    "        \n",
    "        style = list(style_vocab.token2id.keys())[0]\n",
    "        instances = [\n",
    "            {\n",
    "                'sentence': sent,\n",
    "                'style': style,\n",
    "            }\n",
    "            for sent in sentences\n",
    "        ]\n",
    "#         print(\"INST!\")\n",
    "        for inst in instances:\n",
    "#             print(inst)\n",
    "            inst_encoded = dataset_train.encode_instance(inst)\n",
    "            inst.update(inst_encoded)            \n",
    "    \n",
    "    \n",
    "    instances = [\n",
    "        {\n",
    "            'sentence': inst['sentence_enc'],\n",
    "            'style': inst['style_enc'],\n",
    "        } \n",
    "        for inst in instances\n",
    "    ]\n",
    "    \n",
    "    instances = default_collate(instances)\n",
    "    instances = to_device(instances)      \n",
    "    \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(outputs):\n",
    "    predicted_indices = outputs[\"predictions\"]\n",
    "    end_idx = vocab[Vocab.END_TOKEN]\n",
    "    \n",
    "    if not isinstance(predicted_indices, np.ndarray):\n",
    "        predicted_indices = predicted_indices.detach().cpu().numpy()\n",
    "\n",
    "    all_predicted_tokens = []\n",
    "    for indices in predicted_indices:\n",
    "        indices = list(indices)\n",
    "\n",
    "        # Collect indices till the first end_symbol\n",
    "        if end_idx in indices:\n",
    "            indices = indices[:indices.index(end_idx)]\n",
    "\n",
    "        predicted_tokens = [vocab.id2token[x] for x in indices]\n",
    "        all_predicted_tokens.append(predicted_tokens)\n",
    "        \n",
    "    return all_predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence =  ' '.join(dataset_val.instances[1]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they are really good people .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = create_inputs(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': tensor([[136,  55,  29, 380, 368,   8,   2,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0]], device='cuda:0'),\n",
       " 'style': tensor([0], device='cuda:0')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they are really good people .'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swap style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_styles = list(style_vocab.token2id.keys()) #['negative', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['negative', 'positive'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_vocab.token2id.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'positive']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences0 = [s for s in dataset_val.instances if s['style'] == possible_styles[0]]\n",
    "sentences1 = [s for s in dataset_val.instances if s['style'] == possible_styles[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3190 after the discount , they were $ number .\n",
      "3586 anyway , you order your food to the bartenders .\n",
      "1882 i mean really , really bad .\n",
      "2197 apparently the guy was the owner , but he was beyond rude .\n",
      "1968 neither of them seemed to want to help me .\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.choice(np.arange(len(sentences0)), 5):\n",
    "    print(i, ' '.join(sentences0[i]['sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450 we started out with a cheese tasting , which was very good .\n",
      "796 it is pricey , but the food is of high quality and worth it .\n",
      "5455 really good pizza .\n",
      "3798 great experience with this office !\n",
      "1454 my skin looks and feels great and she knew exactly what my skin needed .\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.choice(np.arange(len(sentences1)), 5):\n",
    "    print(i, ' '.join(sentences1[i]['sentence']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3874 the rice had hard things in it .\n"
     ]
    }
   ],
   "source": [
    "print(3874, ' '.join(sentences0[3874]['sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target0 = 3874 # np.random.choice(np.arange(len(sentences0)))\n",
    "target1 = 4935 # np.random.choice(np.arange(len(sentences0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rice had hard things in it .\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(sentences0[target0]['sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which is awesome !\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(sentences1[target1]['sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = create_inputs([\n",
    "    sentences0[target0],\n",
    "    sentences1[target1],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hidden = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder_hidden': tensor([[ 0.9960, -0.1104,  0.3478,  0.1852,  0.5692, -0.3471,  0.3468, -0.1282,\n",
       "          -0.1176,  0.3602,  0.2054, -0.6964,  0.2084, -0.1360, -0.1278, -0.0855,\n",
       "          -0.5349,  0.5455, -0.1412,  0.2628,  0.3365, -0.2249,  0.4589,  0.1841,\n",
       "          -0.0585,  0.5467, -0.3516,  0.9126, -0.3469, -0.8066, -0.6026, -0.4069,\n",
       "          -0.2361, -0.3126, -0.8887, -0.6321, -0.7467,  0.3509, -0.0832,  0.5541,\n",
       "          -0.2939,  0.9630,  0.3044, -0.6743, -0.1846,  0.0799, -0.7861, -0.0899,\n",
       "          -0.0869, -0.1461, -1.2180, -0.1254,  0.5410, -0.2104, -1.0996,  0.2980,\n",
       "          -0.0926, -0.2994, -0.6431, -0.2990,  0.2858, -0.9104,  0.0771, -0.2561,\n",
       "           0.0206,  0.7998,  0.6394, -0.2721,  0.2689, -0.5429, -0.7816, -0.6238,\n",
       "          -0.0152,  0.9413, -0.2934, -0.5155,  0.8586,  0.4707, -0.7585,  0.4288,\n",
       "           0.6684,  0.1305,  0.7862,  0.0903,  0.5864, -0.6920, -0.1550, -0.9478,\n",
       "          -0.7752,  0.2467, -0.3270, -0.7186, -0.0455,  0.3155,  0.2718, -0.4417,\n",
       "           0.1610, -0.2545,  0.5223, -0.6128,  0.0858,  0.3704, -0.1997,  0.0479,\n",
       "          -0.2612, -0.1838,  0.4265,  1.0115, -0.5857,  0.1431, -0.0843,  0.4479,\n",
       "           0.5083, -0.3524, -0.6071, -0.0892, -0.2560,  1.1984, -0.9328, -0.4092,\n",
       "          -0.3299, -0.3639, -0.0074, -0.1895,  0.4387, -0.8230, -0.2382,  0.3887,\n",
       "          -0.7029, -0.8159, -0.5315, -0.5384,  0.0745,  0.4093, -0.4264, -0.2724,\n",
       "           0.3934, -0.1393,  0.2089,  0.2227,  0.1602, -0.7495, -0.0774,  0.6723,\n",
       "          -0.8733,  0.4615, -0.3090, -0.4544,  0.2526, -0.9417,  0.6027, -0.6132,\n",
       "           0.3286,  0.1058,  0.1003,  0.9983,  0.1695, -0.6926,  0.7432, -0.0812,\n",
       "           0.3422,  1.1496, -0.4231,  0.5147,  0.2943, -0.0827, -0.6066,  0.3894,\n",
       "           0.2375, -0.9730,  0.7511,  0.3815,  0.5043,  0.2219,  0.0787,  0.2044,\n",
       "           0.0725, -0.5363,  0.7466,  0.2655,  0.3243, -0.3407,  0.2431,  0.0779,\n",
       "          -0.5066, -0.2362,  0.1398,  0.0551, -0.2295,  0.3106, -0.8649, -0.6743,\n",
       "          -0.4092, -0.6089,  0.5624, -0.5949,  0.0558,  0.1250, -0.3577, -0.2968,\n",
       "           0.2924, -0.4815, -0.1204, -0.3014,  0.9182, -0.0315, -0.1624, -0.5526,\n",
       "           0.2426, -0.2384,  1.8445,  0.2128,  0.0984, -0.0782, -0.2098, -0.7770,\n",
       "          -0.3216,  0.2060,  0.6525,  0.8862, -0.3938,  0.2870,  0.8361, -0.0085,\n",
       "           0.4966, -0.5376,  0.0250, -0.7948, -0.2438,  0.2921,  0.4318, -0.3862,\n",
       "           0.9453,  0.2992, -0.0852, -0.2954, -0.4553, -0.2434, -0.7251,  0.0104,\n",
       "          -0.0859,  0.9109, -0.8177,  0.7063, -1.0823, -0.1277,  0.1179,  1.0441,\n",
       "          -0.2603, -0.2191, -0.3097, -0.0920, -0.5385,  0.0222,  0.3631,  1.2593],\n",
       "         [ 0.3664,  0.2499,  0.3129, -0.0143,  0.8178,  2.0004,  1.3212,  0.1312,\n",
       "           0.1429,  0.9065,  0.3848, -0.0445, -0.3912, -0.1279,  0.3464, -0.4729,\n",
       "           0.7099, -2.2070, -0.8027,  0.2323, -0.5584,  0.2429, -0.3317, -0.3356,\n",
       "          -0.1391,  0.1169, -0.5680,  1.1284, -0.5904, -3.4506,  0.9443, -1.2347,\n",
       "           0.3601, -1.1216,  0.7700, -0.6213, -0.5980,  1.7820,  0.0912,  2.4645,\n",
       "           1.4746,  0.1231,  0.9806,  1.6135,  0.7534,  0.0486,  0.6767, -0.7076,\n",
       "          -0.5999, -0.0420,  1.8394, -0.4134, -0.8582, -0.8793,  2.9480, -0.9450,\n",
       "          -1.4922, -0.1948,  0.1405,  0.1409, -0.1894, -0.8533, -0.0552,  0.6463,\n",
       "          -1.0350,  0.1541,  0.6802, -0.1944, -0.0713, -1.3324, -1.1162,  0.8769,\n",
       "          -0.7394,  0.1237,  0.3905,  0.2219,  0.8339,  2.5401, -0.5636, -0.1140,\n",
       "          -1.2921,  0.2944,  3.3233, -0.4142,  2.2768, -0.0157,  0.7798, -1.1017,\n",
       "           0.1809,  0.0477,  0.7081, -2.3512,  0.8728, -1.2357, -1.2319,  1.0904,\n",
       "           0.5455,  0.1080,  1.0425, -2.3727, -0.5827,  0.1499,  1.1252,  0.0523,\n",
       "           0.7716, -0.3692,  1.0979,  0.9912, -0.7099,  1.7813, -0.2724, -1.1917,\n",
       "          -0.4040, -1.8130, -0.2113,  0.8007, -0.2540,  0.3146,  0.0452,  1.0005,\n",
       "           0.3081, -1.7167,  0.4472, -0.7984, -0.0560, -1.1856,  0.4550, -0.2208,\n",
       "          -0.6364, -0.6312, -0.0680,  0.1031, -0.1457, -0.9463, -0.3921,  1.8091,\n",
       "          -0.0359,  1.7510,  1.8990, -0.5390, -1.3628,  0.1600, -0.4786,  1.9446,\n",
       "          -1.7203, -0.5788,  0.5974, -2.4986,  1.0342,  1.4896,  2.9698, -3.3848,\n",
       "          -0.0820,  0.2793, -0.6146, -2.1241, -0.1066,  0.7602,  2.5793, -1.6181,\n",
       "          -0.3917, -0.5867, -0.7124,  0.8716,  0.6519,  0.8150, -4.0124,  0.1151,\n",
       "           0.4168, -0.9800,  1.1370, -4.6463,  0.3652, -0.2340,  0.9060,  0.3483,\n",
       "           0.3712, -0.1454, -0.0696,  0.1689, -0.1229, -0.0095, -0.8922, -1.4082,\n",
       "          -1.3216, -0.9042,  0.4494,  0.1759,  0.4837, -1.4023,  0.1709,  0.2917,\n",
       "          -0.6060, -0.3107, -2.8848, -2.7061, -0.7606, -0.4852, -1.1580,  0.8769,\n",
       "          -0.6393,  1.7344, -1.4017,  1.1119,  1.0297,  1.1121, -0.9473, -0.3119,\n",
       "           0.1751, -1.4411, -0.9983,  0.1189, -0.3039, -0.2300,  1.0746, -0.8949,\n",
       "           0.3394, -0.5449,  0.5390,  2.5465,  0.1863,  0.7803,  0.6382, -0.1651,\n",
       "           1.2941,  0.1473,  0.0655,  0.4128,  0.2674, -1.3806,  0.3477, -0.4984,\n",
       "           2.8138,  1.0120,  0.4053, -0.3879, -0.4316, -0.2375, -0.0296, -1.0811,\n",
       "           0.8780, -0.1016, -1.1959, -0.6489, -1.5777, -0.2412, -0.1582,  2.3589,\n",
       "           0.2171, -0.4483, -0.2292,  0.7171, -0.2780,  0.2822, -0.4410,  1.4184]],\n",
       "        device='cuda:0', grad_fn=<HardtanhBackward0>),\n",
       " 'meaning_hidden': tensor([[-0.0516,  0.0860,  0.0934,  0.0153, -0.0192, -0.0567, -0.1301,  0.0610,\n",
       "           0.0742,  0.0037,  0.0620, -0.0353,  0.1053, -0.0321,  0.0769, -0.0044,\n",
       "          -0.0142,  0.0558, -0.0172, -0.0675,  0.1250,  0.1142,  0.0200, -0.1156,\n",
       "          -0.1011,  0.0928, -0.0418,  0.0025, -0.0321, -0.0030, -0.1026, -0.0253,\n",
       "          -0.0403,  0.0778, -0.0874, -0.0515,  0.1547,  0.1286,  0.0127,  0.0275,\n",
       "           0.0606,  0.0189, -0.0852,  0.1140,  0.0776, -0.1291,  0.0512, -0.0957,\n",
       "          -0.0236,  0.0007, -0.0187,  0.0779, -0.1481,  0.0799,  0.1106,  0.0225,\n",
       "          -0.0030, -0.0681, -0.0010, -0.0299,  0.0467, -0.0554,  0.0299, -0.0661,\n",
       "          -0.0952, -0.0420,  0.1096, -0.0464,  0.0842,  0.0862,  0.0406, -0.0474,\n",
       "          -0.1037, -0.1390,  0.0514,  0.0335,  0.0429,  0.0875, -0.0487,  0.0358,\n",
       "           0.0114, -0.0285, -0.0324,  0.0327, -0.0417, -0.1015, -0.1088,  0.0259,\n",
       "           0.1717,  0.0808,  0.0613,  0.0395,  0.0255,  0.0357,  0.0376, -0.0779,\n",
       "           0.0272,  0.0483,  0.0076, -0.0821,  0.0535,  0.1087, -0.1090, -0.1533,\n",
       "           0.1542, -0.0389, -0.0904, -0.0310,  0.0704, -0.0795,  0.0319, -0.0615,\n",
       "          -0.1067, -0.0108, -0.0484, -0.0485, -0.0392, -0.1415,  0.1077, -0.0874,\n",
       "          -0.1687,  0.0260, -0.0673, -0.0366, -0.0439,  0.0273,  0.0657, -0.0758,\n",
       "           0.0214, -0.2290,  0.0335,  0.1520, -0.1244, -0.0164,  0.0166, -0.0898,\n",
       "          -0.0192, -0.0167,  0.1396,  0.0036,  0.0489, -0.0105,  0.0623, -0.0563,\n",
       "          -0.1159, -0.0049,  0.1376, -0.0117, -0.0698,  0.0103, -0.0724, -0.0043,\n",
       "           0.0550,  0.0137,  0.0780, -0.0945, -0.1778, -0.0645,  0.0296,  0.0408,\n",
       "          -0.0304, -0.0752, -0.0214,  0.0629,  0.0779, -0.0034,  0.0536,  0.0369,\n",
       "          -0.0129,  0.0364, -0.1115, -0.0817,  0.1959, -0.0216,  0.0793, -0.0174,\n",
       "          -0.1098,  0.0091,  0.1478, -0.0172,  0.0953,  0.0734,  0.0275, -0.0998,\n",
       "          -0.0375, -0.0870,  0.0540,  0.1358,  0.0714, -0.1303,  0.1002, -0.1094,\n",
       "           0.0346,  0.0472, -0.0448,  0.0237, -0.0013,  0.0778,  0.0856, -0.0708,\n",
       "          -0.0296, -0.0036, -0.0258, -0.0865,  0.0575, -0.0465, -0.0278,  0.0599,\n",
       "           0.0203, -0.0903, -0.0561, -0.0821,  0.0345,  0.0292, -0.0170, -0.0894,\n",
       "           0.0365, -0.0032,  0.0198,  0.0993, -0.0779,  0.1123,  0.0184,  0.0641,\n",
       "           0.0528, -0.0572,  0.0146,  0.0136,  0.0317,  0.0018, -0.0282, -0.0873,\n",
       "           0.0108,  0.0578,  0.0405, -0.0890,  0.1427, -0.0767, -0.1147, -0.0190,\n",
       "           0.0216, -0.0447,  0.1010, -0.0307, -0.0251, -0.0251, -0.0131, -0.0692,\n",
       "           0.0615, -0.0975,  0.0807, -0.0150,  0.0980, -0.0461,  0.0015,  0.0630,\n",
       "           0.0087, -0.0106, -0.1449,  0.0479, -0.0743, -0.1121,  0.0146, -0.0991,\n",
       "          -0.0521,  0.0438, -0.0732, -0.0037, -0.1289,  0.0107, -0.0222,  0.0821,\n",
       "           0.0128,  0.1488, -0.1212, -0.0267, -0.0814, -0.0082,  0.0254, -0.0929,\n",
       "           0.0390,  0.0013, -0.1201, -0.0464, -0.1009, -0.0286, -0.0318, -0.0851,\n",
       "          -0.0269,  0.0982,  0.0059, -0.0535, -0.0870,  0.0741,  0.0321, -0.0730,\n",
       "          -0.0635,  0.0802, -0.0919,  0.0080],\n",
       "         [ 0.1151, -0.1436, -0.0410, -0.0718,  0.0400, -0.2186, -0.0190, -0.1394,\n",
       "           0.1942,  0.0577,  0.0691,  0.1039,  0.0104,  0.1910,  0.2270,  0.0861,\n",
       "          -0.0114, -0.1513,  0.0916,  0.1948,  0.4662,  0.2228, -0.1103, -0.3451,\n",
       "           0.1512,  0.1191, -0.2565,  0.0299,  0.1555,  0.0458,  0.0703,  0.1774,\n",
       "           0.0896, -0.1395, -0.2851,  0.0598, -0.0616,  0.3513, -0.0849, -0.0222,\n",
       "          -0.1975,  0.2362, -0.0310,  0.2208,  0.1290, -0.0931,  0.0319,  0.0181,\n",
       "          -0.0430, -0.0250,  0.0980,  0.2651,  0.1079, -0.0962,  0.1475,  0.0539,\n",
       "          -0.0270,  0.0803, -0.0017,  0.0375, -0.0398,  0.0814,  0.0573,  0.1632,\n",
       "          -0.0345,  0.0251, -0.1820, -0.0183,  0.3719, -0.1480, -0.0779,  0.0188,\n",
       "          -0.1145, -0.1644,  0.0566,  0.1003,  0.1201, -0.1276,  0.0022,  0.0692,\n",
       "          -0.0412,  0.1950,  0.1714,  0.1448, -0.2862, -0.1194, -0.2141,  0.0844,\n",
       "           0.1872,  0.0271,  0.2977, -0.0764,  0.0421, -0.0412, -0.1529, -0.0319,\n",
       "           0.0711, -0.0395, -0.2343, -0.1428, -0.1343,  0.0877,  0.1455, -0.0936,\n",
       "           0.3148,  0.1319, -0.0800,  0.0253,  0.0885, -0.2250,  0.0694, -0.1256,\n",
       "           0.0276, -0.0067,  0.1152,  0.1032,  0.0637,  0.0132,  0.2263, -0.1889,\n",
       "          -0.4154, -0.0859, -0.1523, -0.3218,  0.0049, -0.0232,  0.0511, -0.4241,\n",
       "           0.1698, -0.3027, -0.0934,  0.2633, -0.2212, -0.1932,  0.0732, -0.3449,\n",
       "           0.1197, -0.0589,  0.1980, -0.1015, -0.1630, -0.0394,  0.1914, -0.1131,\n",
       "           0.2009,  0.0579,  0.2413,  0.0596, -0.0111, -0.1301,  0.0292, -0.0254,\n",
       "           0.0436,  0.0145,  0.1325, -0.1262,  0.0574, -0.0176, -0.0179,  0.0020,\n",
       "          -0.0155, -0.3665,  0.3167,  0.2688,  0.0870,  0.0044,  0.0884, -0.0892,\n",
       "           0.1442,  0.0827, -0.2114, -0.1628,  0.2485,  0.0817,  0.3258, -0.0686,\n",
       "          -0.1846,  0.0728,  0.1662, -0.1396, -0.1943,  0.0055, -0.1358, -0.2411,\n",
       "           0.1114,  0.0752,  0.0829,  0.2107, -0.0715,  0.0901,  0.1321, -0.0861,\n",
       "          -0.0519, -0.1624, -0.0544,  0.1436, -0.0513, -0.2129,  0.3148, -0.1656,\n",
       "          -0.0580,  0.0552,  0.0243,  0.1360, -0.1599, -0.0190,  0.0658,  0.0222,\n",
       "          -0.0693,  0.0464,  0.0797, -0.1189, -0.0975,  0.0975,  0.0453, -0.1217,\n",
       "          -0.0984, -0.0645,  0.1223, -0.1567, -0.0131, -0.2396,  0.0873,  0.1945,\n",
       "           0.1051,  0.0346, -0.1674,  0.0158, -0.0825, -0.1227, -0.0694, -0.0438,\n",
       "           0.0710,  0.1179, -0.0232,  0.1237,  0.4053, -0.0915, -0.3744, -0.0320,\n",
       "          -0.0217,  0.0069,  0.2231,  0.1825, -0.1250, -0.0793, -0.0932, -0.0094,\n",
       "           0.0143, -0.3653,  0.0006,  0.0330, -0.1502, -0.0571, -0.0776,  0.0943,\n",
       "          -0.1513,  0.0941, -0.3251, -0.1753, -0.0692, -0.4036, -0.1895, -0.0601,\n",
       "           0.1946, -0.0064, -0.2049, -0.1701, -0.3300, -0.0592, -0.0075,  0.0953,\n",
       "          -0.0572, -0.1107, -0.0669, -0.1304, -0.0641,  0.0580,  0.2781,  0.1449,\n",
       "          -0.2054,  0.3128, -0.3238,  0.1125,  0.3059,  0.0229, -0.0823, -0.1176,\n",
       "           0.0667,  0.0102,  0.0439,  0.1207,  0.0945,  0.0069,  0.0595,  0.0351,\n",
       "          -0.0267,  0.0960, -0.0864,  0.0168]], device='cuda:0',\n",
       "        grad_fn=<HardtanhBackward0>),\n",
       " 'style_hidden': tensor([[-3.0278e-02,  5.7690e-02,  1.8444e-02,  5.2809e-02,  8.3219e-03,\n",
       "          -1.4734e-03, -1.0025e-02, -1.3487e-02,  1.0122e-01, -2.3429e-02,\n",
       "          -5.1612e-02,  4.9771e-03, -1.3125e-01, -1.0415e-01, -2.1229e-03,\n",
       "          -2.7296e-02, -3.1238e-02,  1.3486e-01, -2.5367e-02, -5.2643e-02,\n",
       "           9.6954e-03, -1.1067e-01, -7.3016e-02, -6.2897e-02,  9.4011e-02,\n",
       "          -1.1274e-02,  9.2803e-02,  3.6326e-02,  9.8306e-02,  2.0250e-02,\n",
       "           5.1076e-02, -2.3545e-02,  6.4896e-02,  2.0932e-02, -1.2886e-01,\n",
       "           6.2884e-02, -2.9770e-03, -8.5567e-03,  6.9259e-02,  1.8939e-01,\n",
       "           1.5583e-02, -2.7784e-02, -5.1530e-02, -9.6309e-02,  1.7981e-02,\n",
       "          -2.4997e-02,  1.0669e-02,  9.7082e-02, -4.4796e-02,  6.9807e-02,\n",
       "           3.9387e-02, -6.0287e-03, -4.6511e-02, -2.4075e-02, -1.7630e-01,\n",
       "           1.2863e-01, -3.4788e-02, -8.6013e-02, -3.8631e-02, -3.1944e-02,\n",
       "          -5.1651e-02, -1.3084e-01,  1.3996e-01,  1.6018e-01, -8.1668e-02,\n",
       "          -1.5593e-01, -5.2056e-02, -7.2057e-02,  1.5990e-01,  6.3861e-02,\n",
       "           9.4861e-03,  9.2089e-02,  5.4674e-02,  4.5968e-02,  5.8253e-02,\n",
       "           2.5075e-02,  3.4798e-02, -8.4050e-02, -1.3502e-01, -6.0210e-02,\n",
       "          -4.2780e-02, -1.7677e-02, -4.6661e-02, -1.9419e-02, -1.0624e-01,\n",
       "           6.3383e-02,  1.0778e-01, -6.8273e-02,  6.9810e-02, -7.7431e-02,\n",
       "          -7.0931e-02,  4.3522e-02,  7.0709e-02,  1.1301e-01,  8.3382e-02,\n",
       "          -3.6369e-02,  3.6852e-02, -1.0875e-01, -4.3693e-02, -7.0009e-02,\n",
       "          -4.3288e-02, -5.6696e-02, -1.1740e-01,  3.2106e-02, -2.2048e-02,\n",
       "           1.4220e-01,  5.2293e-02,  7.3591e-02,  8.8094e-02,  1.3380e-01,\n",
       "           4.6549e-02,  9.1949e-02,  1.3531e-02, -1.2600e-02, -1.7777e-02,\n",
       "           4.2311e-02, -1.1927e-01, -5.1100e-03,  1.3650e-02, -8.5635e-03,\n",
       "          -2.7728e-02, -5.1861e-02, -1.0498e-02, -1.1801e-03, -3.6316e-02,\n",
       "          -6.7259e-03,  1.1583e-01,  1.2586e-01,  5.9245e-02, -7.7894e-02,\n",
       "          -8.3574e-02, -2.5114e-02, -3.8572e-02,  3.4354e-02,  3.0550e-02,\n",
       "          -8.0873e-03,  3.4195e-02,  6.1597e-02, -8.3876e-02,  3.7393e-02,\n",
       "           4.2087e-02, -1.0882e-01, -1.2178e-01,  1.6858e-01, -9.2240e-03,\n",
       "          -8.7749e-03, -1.0395e-01, -1.3379e-01, -1.1627e-01, -5.8463e-02,\n",
       "           1.0819e-01, -3.8767e-02,  5.5986e-02, -3.6029e-02, -1.2304e-01,\n",
       "          -7.9706e-02, -9.2709e-02,  5.3017e-02,  4.0113e-02,  3.3432e-02,\n",
       "          -3.6938e-04, -8.5393e-02,  3.8772e-02,  4.1077e-02,  9.0787e-02,\n",
       "           1.0424e-01,  1.3063e-01,  6.6189e-02,  6.5952e-02,  7.9878e-02,\n",
       "          -5.4521e-02, -3.5379e-03,  8.7288e-02,  6.8987e-03,  2.3571e-02,\n",
       "          -7.7330e-02, -2.3795e-02,  8.7483e-02, -9.1386e-02,  1.7790e-03,\n",
       "           3.3783e-02,  1.0443e-01,  2.9905e-02, -5.7051e-02,  1.1053e-01,\n",
       "           1.0926e-01,  1.1446e-01, -8.5454e-02,  2.1105e-02, -9.2919e-02,\n",
       "           8.6819e-03, -1.9605e-02, -9.2818e-02,  7.6188e-02, -9.3729e-03,\n",
       "          -1.0737e-01,  8.4994e-03, -1.0342e-02,  1.8299e-02,  2.8178e-02,\n",
       "          -6.1978e-02,  5.1875e-02,  6.4616e-03, -6.3623e-02,  2.4188e-02,\n",
       "           4.6355e-02, -1.2637e-01, -1.1991e-01, -6.3252e-02, -6.2908e-02,\n",
       "           2.1193e-02, -9.4678e-02, -1.5246e-01,  1.4827e-02, -5.3814e-04,\n",
       "          -3.3505e-03,  2.7813e-02,  1.0224e-02, -9.5071e-02,  6.9409e-02,\n",
       "           4.1865e-02,  3.1320e-02, -5.2358e-02, -1.9332e-02,  6.8952e-03,\n",
       "          -7.5656e-02,  6.0803e-02, -1.0319e-02,  7.2046e-02, -5.1510e-02,\n",
       "           6.2337e-02, -1.3105e-01,  7.1948e-02, -4.2839e-02, -9.2988e-02,\n",
       "          -1.1661e-02,  6.0912e-02, -5.6761e-02,  2.8776e-02,  1.3618e-01,\n",
       "          -8.4346e-02,  1.2099e-01,  9.8290e-02,  8.4442e-04, -5.2185e-02,\n",
       "          -1.0204e-01,  9.7847e-02,  6.7294e-02,  1.5247e-02, -7.9342e-03,\n",
       "          -4.7056e-04, -1.0078e-01, -6.1757e-02, -2.9158e-02,  1.1456e-02,\n",
       "           8.6000e-02, -3.4080e-04,  3.4165e-02, -2.1755e-02, -6.9150e-04,\n",
       "           6.0085e-02, -1.4247e-01, -4.5307e-02, -5.2001e-02,  8.6561e-02,\n",
       "           1.0244e-01,  6.8795e-02,  1.6531e-02,  6.0384e-03,  6.6137e-02,\n",
       "          -2.4105e-02,  8.8761e-02,  4.1931e-02, -1.0432e-01,  6.1992e-02,\n",
       "          -5.2058e-02, -2.4281e-02, -7.7126e-04,  6.9275e-02,  1.4712e-01,\n",
       "          -5.3629e-02, -3.6960e-02, -1.4669e-01,  4.3737e-02, -1.6151e-02,\n",
       "           1.2927e-01,  5.9666e-02,  7.5971e-02, -7.8508e-02, -1.9654e-01,\n",
       "          -7.3507e-02,  1.6995e-02,  1.4737e-01,  4.3264e-02, -9.3113e-02,\n",
       "           3.6720e-02, -6.9850e-02, -8.8779e-02, -8.0509e-05,  1.0743e-01],\n",
       "         [-1.8673e-01, -6.8383e-03, -1.0972e-02, -4.1504e-02,  4.5450e-02,\n",
       "           4.2761e-02,  3.2124e-02, -7.7165e-03,  2.6503e-01,  9.2345e-02,\n",
       "          -2.7182e-01,  3.4910e-02,  1.9602e-01, -1.4242e-01,  1.6818e-01,\n",
       "          -1.2083e-01, -1.1202e-01, -1.1750e-01,  4.2902e-03,  9.2927e-02,\n",
       "          -1.1483e-01,  4.1601e-02, -4.2411e-01,  9.0947e-02, -8.4838e-03,\n",
       "          -1.2627e-01,  4.1469e-01,  2.5599e-01,  4.3508e-01,  3.9776e-01,\n",
       "          -1.7019e-01, -6.1436e-02,  1.7236e-01, -2.0614e-02, -1.4754e-03,\n",
       "          -1.3432e-01, -2.9641e-02,  5.2394e-04, -2.9039e-03, -1.2524e-01,\n",
       "           8.1127e-02, -2.3381e-02,  1.8228e-01,  1.4778e-01,  6.8029e-02,\n",
       "           6.5773e-02,  2.3562e-01, -2.4931e-01, -1.6593e-01,  2.1291e-01,\n",
       "          -1.6395e-02, -5.2244e-02,  4.8224e-02, -1.0983e-01,  1.9703e-01,\n",
       "           9.2984e-03, -1.9185e-01,  2.6237e-02,  2.2996e-01,  1.1089e-02,\n",
       "          -1.9774e-01,  1.4980e-01, -1.1461e-01,  4.8494e-02,  8.5271e-02,\n",
       "           1.0738e-01, -2.4062e-01,  3.7794e-01, -1.8434e-01,  1.1110e-02,\n",
       "          -7.5055e-02, -4.6177e-02,  9.6815e-02,  5.3441e-02,  7.1436e-02,\n",
       "           1.3487e-01,  2.0392e-01,  9.0846e-02, -1.6120e-01,  3.4361e-01,\n",
       "           1.7270e-01,  4.0415e-02, -1.2106e-01,  1.8246e-01,  3.1045e-01,\n",
       "          -1.7908e-01,  1.9844e-01, -5.1075e-02,  4.1835e-02, -3.5073e-02,\n",
       "           2.3618e-01,  1.5601e-01, -1.3490e-01, -1.9824e-01, -2.1633e-01,\n",
       "          -6.2465e-03,  7.1184e-02, -8.3875e-02,  8.2747e-02, -2.8336e-01,\n",
       "           4.4249e-03,  2.5649e-01, -1.2644e-01,  9.1683e-02,  2.8344e-02,\n",
       "           1.1490e-01, -1.7818e-01,  1.6401e-01, -1.8352e-01, -1.3104e-01,\n",
       "          -1.9652e-02, -1.2347e-01,  3.1577e-02,  1.6262e-01, -2.5529e-01,\n",
       "          -7.3971e-02, -4.9989e-02,  6.4862e-02,  8.0388e-02,  3.2430e-03,\n",
       "           2.8856e-02,  9.4640e-02,  1.0347e-01, -1.1913e-01, -2.6492e-01,\n",
       "          -1.7301e-01, -1.4588e-01,  2.0005e-01, -2.3572e-01, -9.4336e-02,\n",
       "          -2.0929e-01, -1.0738e-01, -1.7838e-01, -2.1031e-02,  5.3319e-02,\n",
       "          -1.3118e-01, -2.5148e-02,  8.0844e-02, -7.0334e-02, -4.3653e-02,\n",
       "          -6.4491e-02,  8.6427e-02, -7.4982e-02,  7.3839e-02, -1.3995e-01,\n",
       "           8.5040e-03, -4.6480e-02, -4.0477e-02, -2.7086e-01,  2.3726e-01,\n",
       "           5.6514e-02, -4.7246e-03, -1.3618e-01, -8.9273e-02,  9.1502e-02,\n",
       "           1.0172e-01, -1.7634e-01,  1.9439e-02, -6.6545e-02, -6.1542e-02,\n",
       "          -8.5989e-02,  9.8294e-02,  7.2231e-02,  6.6020e-02,  5.9324e-02,\n",
       "           1.7823e-02,  1.1983e-01,  9.3928e-02,  6.9457e-02, -7.3473e-02,\n",
       "          -4.2236e-02, -8.6873e-02,  1.2993e-01,  4.4731e-02, -1.6413e-01,\n",
       "           3.1083e-02,  1.4668e-01,  4.6888e-02,  2.7144e-02,  1.5134e-01,\n",
       "          -1.4226e-01, -2.4197e-01, -2.7270e-02, -1.4963e-01,  1.3354e-01,\n",
       "           1.4415e-01, -1.7468e-01,  8.7353e-02, -2.5465e-01, -5.1361e-03,\n",
       "          -1.7966e-01,  1.1294e-01,  5.1760e-03,  2.5555e-02,  2.0901e-01,\n",
       "           7.8570e-02,  1.0488e-01, -7.2134e-03,  3.3182e-02, -1.2506e-01,\n",
       "          -9.5549e-02, -1.1879e-01,  1.3411e-02, -2.6338e-01,  6.9911e-02,\n",
       "          -1.4768e-02,  1.3346e-01,  1.3946e-01,  4.8535e-02, -3.5046e-01,\n",
       "          -1.1077e-01,  1.4009e-01,  1.7218e-01,  8.2825e-02,  1.1712e-01,\n",
       "           1.0298e-01, -4.7667e-02,  4.0856e-02,  4.6981e-02,  4.7390e-03,\n",
       "          -1.0780e-01,  2.0259e-01, -2.3183e-01, -2.8457e-01,  7.7863e-02,\n",
       "          -5.2006e-02,  2.3247e-03,  1.9698e-01,  4.7268e-02, -2.2711e-02,\n",
       "           4.7397e-02, -3.7185e-01,  1.8514e-01, -1.8245e-01, -2.2073e-01,\n",
       "          -5.9303e-02,  2.4863e-01, -6.0634e-02, -1.8683e-01,  2.7572e-01,\n",
       "           9.0144e-02,  4.2072e-01, -1.1475e-01,  1.3060e-01, -2.6675e-02,\n",
       "          -1.0482e-02, -1.8165e-01, -2.3880e-01,  7.9341e-02, -1.6747e-01,\n",
       "          -3.0622e-02,  1.6844e-01,  4.1378e-02,  9.4616e-02,  9.4553e-02,\n",
       "           2.8878e-01,  4.3966e-03,  1.3216e-03,  4.5799e-02,  6.7966e-02,\n",
       "           3.5462e-01,  1.2977e-01,  1.5001e-01,  2.1213e-02,  2.5707e-01,\n",
       "           2.8019e-01,  2.6373e-01, -1.0863e-01, -2.2194e-02,  5.1562e-02,\n",
       "           1.6521e-01, -2.8015e-01, -4.0440e-02,  1.9637e-01,  5.6778e-02,\n",
       "          -2.7575e-01,  8.4954e-03, -1.1558e-01,  3.1590e-01,  4.5091e-03,\n",
       "           1.9798e-01,  5.3916e-02,  2.5428e-01, -9.0749e-03,  8.9994e-02,\n",
       "           3.5863e-01,  2.1162e-01, -1.5985e-01, -8.0782e-02,  1.7633e-01,\n",
       "           1.5625e-01, -1.7251e-01, -2.1503e-01, -6.1276e-02,  7.3790e-02,\n",
       "           1.3437e-01,  9.3630e-02,  1.9630e-01, -5.5855e-03,  3.0035e-01]],\n",
       "        device='cuda:0', grad_fn=<HardtanhBackward0>),\n",
       " 'logits': tensor([[[-12.5397, -12.5452,  -4.2268,  ...,  -8.6519, -12.0083,  -8.2720],\n",
       "          [ -9.0538,  -9.0128,  -5.3591,  ...,  -5.5835,  -8.7017,  -8.6701],\n",
       "          [-12.7682, -12.7778,  -5.8234,  ..., -11.6527, -13.8091, -12.6297],\n",
       "          ...,\n",
       "          [-11.0016, -11.1065,  10.0830,  ..., -14.0557, -14.8827, -11.4681],\n",
       "          [-10.6382, -10.7388,  10.0829,  ..., -14.0935, -14.1978, -11.4484],\n",
       "          [-10.3292, -10.4278,   9.9092,  ..., -14.1629, -13.7166, -11.4860]],\n",
       " \n",
       "         [[ -8.8980,  -8.7850,  -5.1215,  ...,  -0.7170,  -5.3179,  -2.2933],\n",
       "          [-11.8533, -11.8790,  -5.9339,  ...,  -7.1608,  -9.7720,  -5.6497],\n",
       "          [-12.1186, -12.1288,  -3.9980,  ...,  -7.2657, -13.3467,  -6.1262],\n",
       "          ...,\n",
       "          [ -6.8319,  -6.8326,  11.1633,  ...,  -8.7736,  -9.0207,  -6.7346],\n",
       "          [ -6.6418,  -6.6348,  11.1725,  ...,  -8.6960,  -8.7375,  -6.7755],\n",
       "          [ -6.5164,  -6.5021,  11.0901,  ...,  -8.6483,  -8.6068,  -6.8891]]],\n",
       "        device='cuda:0', grad_fn=<CatBackward>),\n",
       " 'predictions': tensor([[  12, 1313,    5,  454,  689,  216,    8,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2],\n",
       "         [ 281,   16, 1042,  171,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2]], device='cuda:0'),\n",
       " 'loss': tensor(0.8584, device='cuda:0', grad_fn=<NllLossBackward>)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01],\n",
       "        [-1.8673e-01, -6.8383e-03, -1.0972e-02, -4.1504e-02,  4.5450e-02,\n",
       "          4.2761e-02,  3.2124e-02, -7.7165e-03,  2.6503e-01,  9.2345e-02,\n",
       "         -2.7182e-01,  3.4910e-02,  1.9602e-01, -1.4242e-01,  1.6818e-01,\n",
       "         -1.2083e-01, -1.1202e-01, -1.1750e-01,  4.2902e-03,  9.2927e-02,\n",
       "         -1.1483e-01,  4.1601e-02, -4.2411e-01,  9.0947e-02, -8.4838e-03,\n",
       "         -1.2627e-01,  4.1469e-01,  2.5599e-01,  4.3508e-01,  3.9776e-01,\n",
       "         -1.7019e-01, -6.1436e-02,  1.7236e-01, -2.0614e-02, -1.4754e-03,\n",
       "         -1.3432e-01, -2.9641e-02,  5.2394e-04, -2.9039e-03, -1.2524e-01,\n",
       "          8.1127e-02, -2.3381e-02,  1.8228e-01,  1.4778e-01,  6.8029e-02,\n",
       "          6.5773e-02,  2.3562e-01, -2.4931e-01, -1.6593e-01,  2.1291e-01,\n",
       "         -1.6395e-02, -5.2244e-02,  4.8224e-02, -1.0983e-01,  1.9703e-01,\n",
       "          9.2984e-03, -1.9185e-01,  2.6237e-02,  2.2996e-01,  1.1089e-02,\n",
       "         -1.9774e-01,  1.4980e-01, -1.1461e-01,  4.8494e-02,  8.5271e-02,\n",
       "          1.0738e-01, -2.4062e-01,  3.7794e-01, -1.8434e-01,  1.1110e-02,\n",
       "         -7.5055e-02, -4.6177e-02,  9.6815e-02,  5.3441e-02,  7.1436e-02,\n",
       "          1.3487e-01,  2.0392e-01,  9.0846e-02, -1.6120e-01,  3.4361e-01,\n",
       "          1.7270e-01,  4.0415e-02, -1.2106e-01,  1.8246e-01,  3.1045e-01,\n",
       "         -1.7908e-01,  1.9844e-01, -5.1075e-02,  4.1835e-02, -3.5073e-02,\n",
       "          2.3618e-01,  1.5601e-01, -1.3490e-01, -1.9824e-01, -2.1633e-01,\n",
       "         -6.2465e-03,  7.1184e-02, -8.3875e-02,  8.2747e-02, -2.8336e-01,\n",
       "          4.4249e-03,  2.5649e-01, -1.2644e-01,  9.1683e-02,  2.8344e-02,\n",
       "          1.1490e-01, -1.7818e-01,  1.6401e-01, -1.8352e-01, -1.3104e-01,\n",
       "         -1.9652e-02, -1.2347e-01,  3.1577e-02,  1.6262e-01, -2.5529e-01,\n",
       "         -7.3971e-02, -4.9989e-02,  6.4862e-02,  8.0388e-02,  3.2430e-03,\n",
       "          2.8856e-02,  9.4640e-02,  1.0347e-01, -1.1913e-01, -2.6492e-01,\n",
       "         -1.7301e-01, -1.4588e-01,  2.0005e-01, -2.3572e-01, -9.4336e-02,\n",
       "         -2.0929e-01, -1.0738e-01, -1.7838e-01, -2.1031e-02,  5.3319e-02,\n",
       "         -1.3118e-01, -2.5148e-02,  8.0844e-02, -7.0334e-02, -4.3653e-02,\n",
       "         -6.4491e-02,  8.6427e-02, -7.4982e-02,  7.3839e-02, -1.3995e-01,\n",
       "          8.5040e-03, -4.6480e-02, -4.0477e-02, -2.7086e-01,  2.3726e-01,\n",
       "          5.6514e-02, -4.7246e-03, -1.3618e-01, -8.9273e-02,  9.1502e-02,\n",
       "          1.0172e-01, -1.7634e-01,  1.9439e-02, -6.6545e-02, -6.1542e-02,\n",
       "         -8.5989e-02,  9.8294e-02,  7.2231e-02,  6.6020e-02,  5.9324e-02,\n",
       "          1.7823e-02,  1.1983e-01,  9.3928e-02,  6.9457e-02, -7.3473e-02,\n",
       "         -4.2236e-02, -8.6873e-02,  1.2993e-01,  4.4731e-02, -1.6413e-01,\n",
       "          3.1083e-02,  1.4668e-01,  4.6888e-02,  2.7144e-02,  1.5134e-01,\n",
       "         -1.4226e-01, -2.4197e-01, -2.7270e-02, -1.4963e-01,  1.3354e-01,\n",
       "          1.4415e-01, -1.7468e-01,  8.7353e-02, -2.5465e-01, -5.1361e-03,\n",
       "         -1.7966e-01,  1.1294e-01,  5.1760e-03,  2.5555e-02,  2.0901e-01,\n",
       "          7.8570e-02,  1.0488e-01, -7.2134e-03,  3.3182e-02, -1.2506e-01,\n",
       "         -9.5549e-02, -1.1879e-01,  1.3411e-02, -2.6338e-01,  6.9911e-02,\n",
       "         -1.4768e-02,  1.3346e-01,  1.3946e-01,  4.8535e-02, -3.5046e-01,\n",
       "         -1.1077e-01,  1.4009e-01,  1.7218e-01,  8.2825e-02,  1.1712e-01,\n",
       "          1.0298e-01, -4.7667e-02,  4.0856e-02,  4.6981e-02,  4.7390e-03,\n",
       "         -1.0780e-01,  2.0259e-01, -2.3183e-01, -2.8457e-01,  7.7863e-02,\n",
       "         -5.2006e-02,  2.3247e-03,  1.9698e-01,  4.7268e-02, -2.2711e-02,\n",
       "          4.7397e-02, -3.7185e-01,  1.8514e-01, -1.8245e-01, -2.2073e-01,\n",
       "         -5.9303e-02,  2.4863e-01, -6.0634e-02, -1.8683e-01,  2.7572e-01,\n",
       "          9.0144e-02,  4.2072e-01, -1.1475e-01,  1.3060e-01, -2.6675e-02,\n",
       "         -1.0482e-02, -1.8165e-01, -2.3880e-01,  7.9341e-02, -1.6747e-01,\n",
       "         -3.0622e-02,  1.6844e-01,  4.1378e-02,  9.4616e-02,  9.4553e-02,\n",
       "          2.8878e-01,  4.3966e-03,  1.3216e-03,  4.5799e-02,  6.7966e-02,\n",
       "          3.5462e-01,  1.2977e-01,  1.5001e-01,  2.1213e-02,  2.5707e-01,\n",
       "          2.8019e-01,  2.6373e-01, -1.0863e-01, -2.2194e-02,  5.1562e-02,\n",
       "          1.6521e-01, -2.8015e-01, -4.0440e-02,  1.9637e-01,  5.6778e-02,\n",
       "         -2.7575e-01,  8.4954e-03, -1.1558e-01,  3.1590e-01,  4.5091e-03,\n",
       "          1.9798e-01,  5.3916e-02,  2.5428e-01, -9.0749e-03,  8.9994e-02,\n",
       "          3.5863e-01,  2.1162e-01, -1.5985e-01, -8.0782e-02,  1.7633e-01,\n",
       "          1.5625e-01, -1.7251e-01, -2.1503e-01, -6.1276e-02,  7.3790e-02,\n",
       "          1.3437e-01,  9.3630e-02,  1.9630e-01, -5.5855e-03,  3.0035e-01]],\n",
       "       device='cuda:0', grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_hidden['style_hidden']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 300])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_hidden['meaning_hidden'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder_hidden': tensor([[ 0.9960, -0.1104,  0.3478,  0.1852,  0.5692, -0.3471,  0.3468, -0.1282,\n",
       "          -0.1176,  0.3602,  0.2054, -0.6964,  0.2084, -0.1360, -0.1278, -0.0855,\n",
       "          -0.5349,  0.5455, -0.1412,  0.2628,  0.3365, -0.2249,  0.4589,  0.1841,\n",
       "          -0.0585,  0.5467, -0.3516,  0.9126, -0.3469, -0.8066, -0.6026, -0.4069,\n",
       "          -0.2361, -0.3126, -0.8887, -0.6321, -0.7467,  0.3509, -0.0832,  0.5541,\n",
       "          -0.2939,  0.9630,  0.3044, -0.6743, -0.1846,  0.0799, -0.7861, -0.0899,\n",
       "          -0.0869, -0.1461, -1.2180, -0.1254,  0.5410, -0.2104, -1.0996,  0.2980,\n",
       "          -0.0926, -0.2994, -0.6431, -0.2990,  0.2858, -0.9104,  0.0771, -0.2561,\n",
       "           0.0206,  0.7998,  0.6394, -0.2721,  0.2689, -0.5429, -0.7816, -0.6238,\n",
       "          -0.0152,  0.9413, -0.2934, -0.5155,  0.8586,  0.4707, -0.7585,  0.4288,\n",
       "           0.6684,  0.1305,  0.7862,  0.0903,  0.5864, -0.6920, -0.1550, -0.9478,\n",
       "          -0.7752,  0.2467, -0.3270, -0.7186, -0.0455,  0.3155,  0.2718, -0.4417,\n",
       "           0.1610, -0.2545,  0.5223, -0.6128,  0.0858,  0.3704, -0.1997,  0.0479,\n",
       "          -0.2612, -0.1838,  0.4265,  1.0115, -0.5857,  0.1431, -0.0843,  0.4479,\n",
       "           0.5083, -0.3524, -0.6071, -0.0892, -0.2560,  1.1984, -0.9328, -0.4092,\n",
       "          -0.3299, -0.3639, -0.0074, -0.1895,  0.4387, -0.8230, -0.2382,  0.3887,\n",
       "          -0.7029, -0.8159, -0.5315, -0.5384,  0.0745,  0.4093, -0.4264, -0.2724,\n",
       "           0.3934, -0.1393,  0.2089,  0.2227,  0.1602, -0.7495, -0.0774,  0.6723,\n",
       "          -0.8733,  0.4615, -0.3090, -0.4544,  0.2526, -0.9417,  0.6027, -0.6132,\n",
       "           0.3286,  0.1058,  0.1003,  0.9983,  0.1695, -0.6926,  0.7432, -0.0812,\n",
       "           0.3422,  1.1496, -0.4231,  0.5147,  0.2943, -0.0827, -0.6066,  0.3894,\n",
       "           0.2375, -0.9730,  0.7511,  0.3815,  0.5043,  0.2219,  0.0787,  0.2044,\n",
       "           0.0725, -0.5363,  0.7466,  0.2655,  0.3243, -0.3407,  0.2431,  0.0779,\n",
       "          -0.5066, -0.2362,  0.1398,  0.0551, -0.2295,  0.3106, -0.8649, -0.6743,\n",
       "          -0.4092, -0.6089,  0.5624, -0.5949,  0.0558,  0.1250, -0.3577, -0.2968,\n",
       "           0.2924, -0.4815, -0.1204, -0.3014,  0.9182, -0.0315, -0.1624, -0.5526,\n",
       "           0.2426, -0.2384,  1.8445,  0.2128,  0.0984, -0.0782, -0.2098, -0.7770,\n",
       "          -0.3216,  0.2060,  0.6525,  0.8862, -0.3938,  0.2870,  0.8361, -0.0085,\n",
       "           0.4966, -0.5376,  0.0250, -0.7948, -0.2438,  0.2921,  0.4318, -0.3862,\n",
       "           0.9453,  0.2992, -0.0852, -0.2954, -0.4553, -0.2434, -0.7251,  0.0104,\n",
       "          -0.0859,  0.9109, -0.8177,  0.7063, -1.0823, -0.1277,  0.1179,  1.0441,\n",
       "          -0.2603, -0.2191, -0.3097, -0.0920, -0.5385,  0.0222,  0.3631,  1.2593],\n",
       "         [ 0.3664,  0.2499,  0.3129, -0.0143,  0.8178,  2.0004,  1.3212,  0.1312,\n",
       "           0.1429,  0.9065,  0.3848, -0.0445, -0.3912, -0.1279,  0.3464, -0.4729,\n",
       "           0.7099, -2.2070, -0.8027,  0.2323, -0.5584,  0.2429, -0.3317, -0.3356,\n",
       "          -0.1391,  0.1169, -0.5680,  1.1284, -0.5904, -3.4506,  0.9443, -1.2347,\n",
       "           0.3601, -1.1216,  0.7700, -0.6213, -0.5980,  1.7820,  0.0912,  2.4645,\n",
       "           1.4746,  0.1231,  0.9806,  1.6135,  0.7534,  0.0486,  0.6767, -0.7076,\n",
       "          -0.5999, -0.0420,  1.8394, -0.4134, -0.8582, -0.8793,  2.9480, -0.9450,\n",
       "          -1.4922, -0.1948,  0.1405,  0.1409, -0.1894, -0.8533, -0.0552,  0.6463,\n",
       "          -1.0350,  0.1541,  0.6802, -0.1944, -0.0713, -1.3324, -1.1162,  0.8769,\n",
       "          -0.7394,  0.1237,  0.3905,  0.2219,  0.8339,  2.5401, -0.5636, -0.1140,\n",
       "          -1.2921,  0.2944,  3.3233, -0.4142,  2.2768, -0.0157,  0.7798, -1.1017,\n",
       "           0.1809,  0.0477,  0.7081, -2.3512,  0.8728, -1.2357, -1.2319,  1.0904,\n",
       "           0.5455,  0.1080,  1.0425, -2.3727, -0.5827,  0.1499,  1.1252,  0.0523,\n",
       "           0.7716, -0.3692,  1.0979,  0.9912, -0.7099,  1.7813, -0.2724, -1.1917,\n",
       "          -0.4040, -1.8130, -0.2113,  0.8007, -0.2540,  0.3146,  0.0452,  1.0005,\n",
       "           0.3081, -1.7167,  0.4472, -0.7984, -0.0560, -1.1856,  0.4550, -0.2208,\n",
       "          -0.6364, -0.6312, -0.0680,  0.1031, -0.1457, -0.9463, -0.3921,  1.8091,\n",
       "          -0.0359,  1.7510,  1.8990, -0.5390, -1.3628,  0.1600, -0.4786,  1.9446,\n",
       "          -1.7203, -0.5788,  0.5974, -2.4986,  1.0342,  1.4896,  2.9698, -3.3848,\n",
       "          -0.0820,  0.2793, -0.6146, -2.1241, -0.1066,  0.7602,  2.5793, -1.6181,\n",
       "          -0.3917, -0.5867, -0.7124,  0.8716,  0.6519,  0.8150, -4.0124,  0.1151,\n",
       "           0.4168, -0.9800,  1.1370, -4.6463,  0.3652, -0.2340,  0.9060,  0.3483,\n",
       "           0.3712, -0.1454, -0.0696,  0.1689, -0.1229, -0.0095, -0.8922, -1.4082,\n",
       "          -1.3216, -0.9042,  0.4494,  0.1759,  0.4837, -1.4023,  0.1709,  0.2917,\n",
       "          -0.6060, -0.3107, -2.8848, -2.7061, -0.7606, -0.4852, -1.1580,  0.8769,\n",
       "          -0.6393,  1.7344, -1.4017,  1.1119,  1.0297,  1.1121, -0.9473, -0.3119,\n",
       "           0.1751, -1.4411, -0.9983,  0.1189, -0.3039, -0.2300,  1.0746, -0.8949,\n",
       "           0.3394, -0.5449,  0.5390,  2.5465,  0.1863,  0.7803,  0.6382, -0.1651,\n",
       "           1.2941,  0.1473,  0.0655,  0.4128,  0.2674, -1.3806,  0.3477, -0.4984,\n",
       "           2.8138,  1.0120,  0.4053, -0.3879, -0.4316, -0.2375, -0.0296, -1.0811,\n",
       "           0.8780, -0.1016, -1.1959, -0.6489, -1.5777, -0.2412, -0.1582,  2.3589,\n",
       "           0.2171, -0.4483, -0.2292,  0.7171, -0.2780,  0.2822, -0.4410,  1.4184]],\n",
       "        device='cuda:0', grad_fn=<HardtanhBackward0>),\n",
       " 'meaning_hidden': tensor([[-0.0516,  0.0860,  0.0934,  0.0153, -0.0192, -0.0567, -0.1301,  0.0610,\n",
       "           0.0742,  0.0037,  0.0620, -0.0353,  0.1053, -0.0321,  0.0769, -0.0044,\n",
       "          -0.0142,  0.0558, -0.0172, -0.0675,  0.1250,  0.1142,  0.0200, -0.1156,\n",
       "          -0.1011,  0.0928, -0.0418,  0.0025, -0.0321, -0.0030, -0.1026, -0.0253,\n",
       "          -0.0403,  0.0778, -0.0874, -0.0515,  0.1547,  0.1286,  0.0127,  0.0275,\n",
       "           0.0606,  0.0189, -0.0852,  0.1140,  0.0776, -0.1291,  0.0512, -0.0957,\n",
       "          -0.0236,  0.0007, -0.0187,  0.0779, -0.1481,  0.0799,  0.1106,  0.0225,\n",
       "          -0.0030, -0.0681, -0.0010, -0.0299,  0.0467, -0.0554,  0.0299, -0.0661,\n",
       "          -0.0952, -0.0420,  0.1096, -0.0464,  0.0842,  0.0862,  0.0406, -0.0474,\n",
       "          -0.1037, -0.1390,  0.0514,  0.0335,  0.0429,  0.0875, -0.0487,  0.0358,\n",
       "           0.0114, -0.0285, -0.0324,  0.0327, -0.0417, -0.1015, -0.1088,  0.0259,\n",
       "           0.1717,  0.0808,  0.0613,  0.0395,  0.0255,  0.0357,  0.0376, -0.0779,\n",
       "           0.0272,  0.0483,  0.0076, -0.0821,  0.0535,  0.1087, -0.1090, -0.1533,\n",
       "           0.1542, -0.0389, -0.0904, -0.0310,  0.0704, -0.0795,  0.0319, -0.0615,\n",
       "          -0.1067, -0.0108, -0.0484, -0.0485, -0.0392, -0.1415,  0.1077, -0.0874,\n",
       "          -0.1687,  0.0260, -0.0673, -0.0366, -0.0439,  0.0273,  0.0657, -0.0758,\n",
       "           0.0214, -0.2290,  0.0335,  0.1520, -0.1244, -0.0164,  0.0166, -0.0898,\n",
       "          -0.0192, -0.0167,  0.1396,  0.0036,  0.0489, -0.0105,  0.0623, -0.0563,\n",
       "          -0.1159, -0.0049,  0.1376, -0.0117, -0.0698,  0.0103, -0.0724, -0.0043,\n",
       "           0.0550,  0.0137,  0.0780, -0.0945, -0.1778, -0.0645,  0.0296,  0.0408,\n",
       "          -0.0304, -0.0752, -0.0214,  0.0629,  0.0779, -0.0034,  0.0536,  0.0369,\n",
       "          -0.0129,  0.0364, -0.1115, -0.0817,  0.1959, -0.0216,  0.0793, -0.0174,\n",
       "          -0.1098,  0.0091,  0.1478, -0.0172,  0.0953,  0.0734,  0.0275, -0.0998,\n",
       "          -0.0375, -0.0870,  0.0540,  0.1358,  0.0714, -0.1303,  0.1002, -0.1094,\n",
       "           0.0346,  0.0472, -0.0448,  0.0237, -0.0013,  0.0778,  0.0856, -0.0708,\n",
       "          -0.0296, -0.0036, -0.0258, -0.0865,  0.0575, -0.0465, -0.0278,  0.0599,\n",
       "           0.0203, -0.0903, -0.0561, -0.0821,  0.0345,  0.0292, -0.0170, -0.0894,\n",
       "           0.0365, -0.0032,  0.0198,  0.0993, -0.0779,  0.1123,  0.0184,  0.0641,\n",
       "           0.0528, -0.0572,  0.0146,  0.0136,  0.0317,  0.0018, -0.0282, -0.0873,\n",
       "           0.0108,  0.0578,  0.0405, -0.0890,  0.1427, -0.0767, -0.1147, -0.0190,\n",
       "           0.0216, -0.0447,  0.1010, -0.0307, -0.0251, -0.0251, -0.0131, -0.0692,\n",
       "           0.0615, -0.0975,  0.0807, -0.0150,  0.0980, -0.0461,  0.0015,  0.0630,\n",
       "           0.0087, -0.0106, -0.1449,  0.0479, -0.0743, -0.1121,  0.0146, -0.0991,\n",
       "          -0.0521,  0.0438, -0.0732, -0.0037, -0.1289,  0.0107, -0.0222,  0.0821,\n",
       "           0.0128,  0.1488, -0.1212, -0.0267, -0.0814, -0.0082,  0.0254, -0.0929,\n",
       "           0.0390,  0.0013, -0.1201, -0.0464, -0.1009, -0.0286, -0.0318, -0.0851,\n",
       "          -0.0269,  0.0982,  0.0059, -0.0535, -0.0870,  0.0741,  0.0321, -0.0730,\n",
       "          -0.0635,  0.0802, -0.0919,  0.0080],\n",
       "         [ 0.1151, -0.1436, -0.0410, -0.0718,  0.0400, -0.2186, -0.0190, -0.1394,\n",
       "           0.1942,  0.0577,  0.0691,  0.1039,  0.0104,  0.1910,  0.2270,  0.0861,\n",
       "          -0.0114, -0.1513,  0.0916,  0.1948,  0.4662,  0.2228, -0.1103, -0.3451,\n",
       "           0.1512,  0.1191, -0.2565,  0.0299,  0.1555,  0.0458,  0.0703,  0.1774,\n",
       "           0.0896, -0.1395, -0.2851,  0.0598, -0.0616,  0.3513, -0.0849, -0.0222,\n",
       "          -0.1975,  0.2362, -0.0310,  0.2208,  0.1290, -0.0931,  0.0319,  0.0181,\n",
       "          -0.0430, -0.0250,  0.0980,  0.2651,  0.1079, -0.0962,  0.1475,  0.0539,\n",
       "          -0.0270,  0.0803, -0.0017,  0.0375, -0.0398,  0.0814,  0.0573,  0.1632,\n",
       "          -0.0345,  0.0251, -0.1820, -0.0183,  0.3719, -0.1480, -0.0779,  0.0188,\n",
       "          -0.1145, -0.1644,  0.0566,  0.1003,  0.1201, -0.1276,  0.0022,  0.0692,\n",
       "          -0.0412,  0.1950,  0.1714,  0.1448, -0.2862, -0.1194, -0.2141,  0.0844,\n",
       "           0.1872,  0.0271,  0.2977, -0.0764,  0.0421, -0.0412, -0.1529, -0.0319,\n",
       "           0.0711, -0.0395, -0.2343, -0.1428, -0.1343,  0.0877,  0.1455, -0.0936,\n",
       "           0.3148,  0.1319, -0.0800,  0.0253,  0.0885, -0.2250,  0.0694, -0.1256,\n",
       "           0.0276, -0.0067,  0.1152,  0.1032,  0.0637,  0.0132,  0.2263, -0.1889,\n",
       "          -0.4154, -0.0859, -0.1523, -0.3218,  0.0049, -0.0232,  0.0511, -0.4241,\n",
       "           0.1698, -0.3027, -0.0934,  0.2633, -0.2212, -0.1932,  0.0732, -0.3449,\n",
       "           0.1197, -0.0589,  0.1980, -0.1015, -0.1630, -0.0394,  0.1914, -0.1131,\n",
       "           0.2009,  0.0579,  0.2413,  0.0596, -0.0111, -0.1301,  0.0292, -0.0254,\n",
       "           0.0436,  0.0145,  0.1325, -0.1262,  0.0574, -0.0176, -0.0179,  0.0020,\n",
       "          -0.0155, -0.3665,  0.3167,  0.2688,  0.0870,  0.0044,  0.0884, -0.0892,\n",
       "           0.1442,  0.0827, -0.2114, -0.1628,  0.2485,  0.0817,  0.3258, -0.0686,\n",
       "          -0.1846,  0.0728,  0.1662, -0.1396, -0.1943,  0.0055, -0.1358, -0.2411,\n",
       "           0.1114,  0.0752,  0.0829,  0.2107, -0.0715,  0.0901,  0.1321, -0.0861,\n",
       "          -0.0519, -0.1624, -0.0544,  0.1436, -0.0513, -0.2129,  0.3148, -0.1656,\n",
       "          -0.0580,  0.0552,  0.0243,  0.1360, -0.1599, -0.0190,  0.0658,  0.0222,\n",
       "          -0.0693,  0.0464,  0.0797, -0.1189, -0.0975,  0.0975,  0.0453, -0.1217,\n",
       "          -0.0984, -0.0645,  0.1223, -0.1567, -0.0131, -0.2396,  0.0873,  0.1945,\n",
       "           0.1051,  0.0346, -0.1674,  0.0158, -0.0825, -0.1227, -0.0694, -0.0438,\n",
       "           0.0710,  0.1179, -0.0232,  0.1237,  0.4053, -0.0915, -0.3744, -0.0320,\n",
       "          -0.0217,  0.0069,  0.2231,  0.1825, -0.1250, -0.0793, -0.0932, -0.0094,\n",
       "           0.0143, -0.3653,  0.0006,  0.0330, -0.1502, -0.0571, -0.0776,  0.0943,\n",
       "          -0.1513,  0.0941, -0.3251, -0.1753, -0.0692, -0.4036, -0.1895, -0.0601,\n",
       "           0.1946, -0.0064, -0.2049, -0.1701, -0.3300, -0.0592, -0.0075,  0.0953,\n",
       "          -0.0572, -0.1107, -0.0669, -0.1304, -0.0641,  0.0580,  0.2781,  0.1449,\n",
       "          -0.2054,  0.3128, -0.3238,  0.1125,  0.3059,  0.0229, -0.0823, -0.1176,\n",
       "           0.0667,  0.0102,  0.0439,  0.1207,  0.0945,  0.0069,  0.0595,  0.0351,\n",
       "          -0.0267,  0.0960, -0.0864,  0.0168]], device='cuda:0',\n",
       "        grad_fn=<HardtanhBackward0>),\n",
       " 'style_hidden': tensor([[ 9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01],\n",
       "         [-1.8673e-01, -6.8383e-03, -1.0972e-02, -4.1504e-02,  4.5450e-02,\n",
       "           4.2761e-02,  3.2124e-02, -7.7165e-03,  2.6503e-01,  9.2345e-02,\n",
       "          -2.7182e-01,  3.4910e-02,  1.9602e-01, -1.4242e-01,  1.6818e-01,\n",
       "          -1.2083e-01, -1.1202e-01, -1.1750e-01,  4.2902e-03,  9.2927e-02,\n",
       "          -1.1483e-01,  4.1601e-02, -4.2411e-01,  9.0947e-02, -8.4838e-03,\n",
       "          -1.2627e-01,  4.1469e-01,  2.5599e-01,  4.3508e-01,  3.9776e-01,\n",
       "          -1.7019e-01, -6.1436e-02,  1.7236e-01, -2.0614e-02, -1.4754e-03,\n",
       "          -1.3432e-01, -2.9641e-02,  5.2394e-04, -2.9039e-03, -1.2524e-01,\n",
       "           8.1127e-02, -2.3381e-02,  1.8228e-01,  1.4778e-01,  6.8029e-02,\n",
       "           6.5773e-02,  2.3562e-01, -2.4931e-01, -1.6593e-01,  2.1291e-01,\n",
       "          -1.6395e-02, -5.2244e-02,  4.8224e-02, -1.0983e-01,  1.9703e-01,\n",
       "           9.2984e-03, -1.9185e-01,  2.6237e-02,  2.2996e-01,  1.1089e-02,\n",
       "          -1.9774e-01,  1.4980e-01, -1.1461e-01,  4.8494e-02,  8.5271e-02,\n",
       "           1.0738e-01, -2.4062e-01,  3.7794e-01, -1.8434e-01,  1.1110e-02,\n",
       "          -7.5055e-02, -4.6177e-02,  9.6815e-02,  5.3441e-02,  7.1436e-02,\n",
       "           1.3487e-01,  2.0392e-01,  9.0846e-02, -1.6120e-01,  3.4361e-01,\n",
       "           1.7270e-01,  4.0415e-02, -1.2106e-01,  1.8246e-01,  3.1045e-01,\n",
       "          -1.7908e-01,  1.9844e-01, -5.1075e-02,  4.1835e-02, -3.5073e-02,\n",
       "           2.3618e-01,  1.5601e-01, -1.3490e-01, -1.9824e-01, -2.1633e-01,\n",
       "          -6.2465e-03,  7.1184e-02, -8.3875e-02,  8.2747e-02, -2.8336e-01,\n",
       "           4.4249e-03,  2.5649e-01, -1.2644e-01,  9.1683e-02,  2.8344e-02,\n",
       "           1.1490e-01, -1.7818e-01,  1.6401e-01, -1.8352e-01, -1.3104e-01,\n",
       "          -1.9652e-02, -1.2347e-01,  3.1577e-02,  1.6262e-01, -2.5529e-01,\n",
       "          -7.3971e-02, -4.9989e-02,  6.4862e-02,  8.0388e-02,  3.2430e-03,\n",
       "           2.8856e-02,  9.4640e-02,  1.0347e-01, -1.1913e-01, -2.6492e-01,\n",
       "          -1.7301e-01, -1.4588e-01,  2.0005e-01, -2.3572e-01, -9.4336e-02,\n",
       "          -2.0929e-01, -1.0738e-01, -1.7838e-01, -2.1031e-02,  5.3319e-02,\n",
       "          -1.3118e-01, -2.5148e-02,  8.0844e-02, -7.0334e-02, -4.3653e-02,\n",
       "          -6.4491e-02,  8.6427e-02, -7.4982e-02,  7.3839e-02, -1.3995e-01,\n",
       "           8.5040e-03, -4.6480e-02, -4.0477e-02, -2.7086e-01,  2.3726e-01,\n",
       "           5.6514e-02, -4.7246e-03, -1.3618e-01, -8.9273e-02,  9.1502e-02,\n",
       "           1.0172e-01, -1.7634e-01,  1.9439e-02, -6.6545e-02, -6.1542e-02,\n",
       "          -8.5989e-02,  9.8294e-02,  7.2231e-02,  6.6020e-02,  5.9324e-02,\n",
       "           1.7823e-02,  1.1983e-01,  9.3928e-02,  6.9457e-02, -7.3473e-02,\n",
       "          -4.2236e-02, -8.6873e-02,  1.2993e-01,  4.4731e-02, -1.6413e-01,\n",
       "           3.1083e-02,  1.4668e-01,  4.6888e-02,  2.7144e-02,  1.5134e-01,\n",
       "          -1.4226e-01, -2.4197e-01, -2.7270e-02, -1.4963e-01,  1.3354e-01,\n",
       "           1.4415e-01, -1.7468e-01,  8.7353e-02, -2.5465e-01, -5.1361e-03,\n",
       "          -1.7966e-01,  1.1294e-01,  5.1760e-03,  2.5555e-02,  2.0901e-01,\n",
       "           7.8570e-02,  1.0488e-01, -7.2134e-03,  3.3182e-02, -1.2506e-01,\n",
       "          -9.5549e-02, -1.1879e-01,  1.3411e-02, -2.6338e-01,  6.9911e-02,\n",
       "          -1.4768e-02,  1.3346e-01,  1.3946e-01,  4.8535e-02, -3.5046e-01,\n",
       "          -1.1077e-01,  1.4009e-01,  1.7218e-01,  8.2825e-02,  1.1712e-01,\n",
       "           1.0298e-01, -4.7667e-02,  4.0856e-02,  4.6981e-02,  4.7390e-03,\n",
       "          -1.0780e-01,  2.0259e-01, -2.3183e-01, -2.8457e-01,  7.7863e-02,\n",
       "          -5.2006e-02,  2.3247e-03,  1.9698e-01,  4.7268e-02, -2.2711e-02,\n",
       "           4.7397e-02, -3.7185e-01,  1.8514e-01, -1.8245e-01, -2.2073e-01,\n",
       "          -5.9303e-02,  2.4863e-01, -6.0634e-02, -1.8683e-01,  2.7572e-01,\n",
       "           9.0144e-02,  4.2072e-01, -1.1475e-01,  1.3060e-01, -2.6675e-02,\n",
       "          -1.0482e-02, -1.8165e-01, -2.3880e-01,  7.9341e-02, -1.6747e-01,\n",
       "          -3.0622e-02,  1.6844e-01,  4.1378e-02,  9.4616e-02,  9.4553e-02,\n",
       "           2.8878e-01,  4.3966e-03,  1.3216e-03,  4.5799e-02,  6.7966e-02,\n",
       "           3.5462e-01,  1.2977e-01,  1.5001e-01,  2.1213e-02,  2.5707e-01,\n",
       "           2.8019e-01,  2.6373e-01, -1.0863e-01, -2.2194e-02,  5.1562e-02,\n",
       "           1.6521e-01, -2.8015e-01, -4.0440e-02,  1.9637e-01,  5.6778e-02,\n",
       "          -2.7575e-01,  8.4954e-03, -1.1558e-01,  3.1590e-01,  4.5091e-03,\n",
       "           1.9798e-01,  5.3916e-02,  2.5428e-01, -9.0749e-03,  8.9994e-02,\n",
       "           3.5863e-01,  2.1162e-01, -1.5985e-01, -8.0782e-02,  1.7633e-01,\n",
       "           1.5625e-01, -1.7251e-01, -2.1503e-01, -6.1276e-02,  7.3790e-02,\n",
       "           1.3437e-01,  9.3630e-02,  1.9630e-01, -5.5855e-03,  3.0035e-01]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " 'logits': tensor([[[-12.5397, -12.5452,  -4.2268,  ...,  -8.6519, -12.0083,  -8.2720],\n",
       "          [ -9.0538,  -9.0128,  -5.3591,  ...,  -5.5835,  -8.7017,  -8.6701],\n",
       "          [-12.7682, -12.7778,  -5.8234,  ..., -11.6527, -13.8091, -12.6297],\n",
       "          ...,\n",
       "          [-11.0016, -11.1065,  10.0830,  ..., -14.0557, -14.8827, -11.4681],\n",
       "          [-10.6382, -10.7388,  10.0829,  ..., -14.0935, -14.1978, -11.4484],\n",
       "          [-10.3292, -10.4278,   9.9092,  ..., -14.1629, -13.7166, -11.4860]],\n",
       " \n",
       "         [[ -8.8980,  -8.7850,  -5.1215,  ...,  -0.7170,  -5.3179,  -2.2933],\n",
       "          [-11.8533, -11.8790,  -5.9339,  ...,  -7.1608,  -9.7720,  -5.6497],\n",
       "          [-12.1186, -12.1288,  -3.9980,  ...,  -7.2657, -13.3467,  -6.1262],\n",
       "          ...,\n",
       "          [ -6.8319,  -6.8326,  11.1633,  ...,  -8.7736,  -9.0207,  -6.7346],\n",
       "          [ -6.6418,  -6.6348,  11.1725,  ...,  -8.6960,  -8.7375,  -6.7755],\n",
       "          [ -6.5164,  -6.5021,  11.0901,  ...,  -8.6483,  -8.6068,  -6.8891]]],\n",
       "        device='cuda:0', grad_fn=<CatBackward>),\n",
       " 'predictions': tensor([[  12, 1313,    5,  454,  689,  216,    8,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2],\n",
       "         [ 281,   16, 1042,  171,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2]], device='cuda:0'),\n",
       " 'loss': tensor(0.8584, device='cuda:0', grad_fn=<NllLossBackward>)}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_decoded = model.decode(z_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder_hidden': tensor([[ 0.9960, -0.1104,  0.3478,  0.1852,  0.5692, -0.3471,  0.3468, -0.1282,\n",
       "          -0.1176,  0.3602,  0.2054, -0.6964,  0.2084, -0.1360, -0.1278, -0.0855,\n",
       "          -0.5349,  0.5455, -0.1412,  0.2628,  0.3365, -0.2249,  0.4589,  0.1841,\n",
       "          -0.0585,  0.5467, -0.3516,  0.9126, -0.3469, -0.8066, -0.6026, -0.4069,\n",
       "          -0.2361, -0.3126, -0.8887, -0.6321, -0.7467,  0.3509, -0.0832,  0.5541,\n",
       "          -0.2939,  0.9630,  0.3044, -0.6743, -0.1846,  0.0799, -0.7861, -0.0899,\n",
       "          -0.0869, -0.1461, -1.2180, -0.1254,  0.5410, -0.2104, -1.0996,  0.2980,\n",
       "          -0.0926, -0.2994, -0.6431, -0.2990,  0.2858, -0.9104,  0.0771, -0.2561,\n",
       "           0.0206,  0.7998,  0.6394, -0.2721,  0.2689, -0.5429, -0.7816, -0.6238,\n",
       "          -0.0152,  0.9413, -0.2934, -0.5155,  0.8586,  0.4707, -0.7585,  0.4288,\n",
       "           0.6684,  0.1305,  0.7862,  0.0903,  0.5864, -0.6920, -0.1550, -0.9478,\n",
       "          -0.7752,  0.2467, -0.3270, -0.7186, -0.0455,  0.3155,  0.2718, -0.4417,\n",
       "           0.1610, -0.2545,  0.5223, -0.6128,  0.0858,  0.3704, -0.1997,  0.0479,\n",
       "          -0.2612, -0.1838,  0.4265,  1.0115, -0.5857,  0.1431, -0.0843,  0.4479,\n",
       "           0.5083, -0.3524, -0.6071, -0.0892, -0.2560,  1.1984, -0.9328, -0.4092,\n",
       "          -0.3299, -0.3639, -0.0074, -0.1895,  0.4387, -0.8230, -0.2382,  0.3887,\n",
       "          -0.7029, -0.8159, -0.5315, -0.5384,  0.0745,  0.4093, -0.4264, -0.2724,\n",
       "           0.3934, -0.1393,  0.2089,  0.2227,  0.1602, -0.7495, -0.0774,  0.6723,\n",
       "          -0.8733,  0.4615, -0.3090, -0.4544,  0.2526, -0.9417,  0.6027, -0.6132,\n",
       "           0.3286,  0.1058,  0.1003,  0.9983,  0.1695, -0.6926,  0.7432, -0.0812,\n",
       "           0.3422,  1.1496, -0.4231,  0.5147,  0.2943, -0.0827, -0.6066,  0.3894,\n",
       "           0.2375, -0.9730,  0.7511,  0.3815,  0.5043,  0.2219,  0.0787,  0.2044,\n",
       "           0.0725, -0.5363,  0.7466,  0.2655,  0.3243, -0.3407,  0.2431,  0.0779,\n",
       "          -0.5066, -0.2362,  0.1398,  0.0551, -0.2295,  0.3106, -0.8649, -0.6743,\n",
       "          -0.4092, -0.6089,  0.5624, -0.5949,  0.0558,  0.1250, -0.3577, -0.2968,\n",
       "           0.2924, -0.4815, -0.1204, -0.3014,  0.9182, -0.0315, -0.1624, -0.5526,\n",
       "           0.2426, -0.2384,  1.8445,  0.2128,  0.0984, -0.0782, -0.2098, -0.7770,\n",
       "          -0.3216,  0.2060,  0.6525,  0.8862, -0.3938,  0.2870,  0.8361, -0.0085,\n",
       "           0.4966, -0.5376,  0.0250, -0.7948, -0.2438,  0.2921,  0.4318, -0.3862,\n",
       "           0.9453,  0.2992, -0.0852, -0.2954, -0.4553, -0.2434, -0.7251,  0.0104,\n",
       "          -0.0859,  0.9109, -0.8177,  0.7063, -1.0823, -0.1277,  0.1179,  1.0441,\n",
       "          -0.2603, -0.2191, -0.3097, -0.0920, -0.5385,  0.0222,  0.3631,  1.2593],\n",
       "         [ 0.3664,  0.2499,  0.3129, -0.0143,  0.8178,  2.0004,  1.3212,  0.1312,\n",
       "           0.1429,  0.9065,  0.3848, -0.0445, -0.3912, -0.1279,  0.3464, -0.4729,\n",
       "           0.7099, -2.2070, -0.8027,  0.2323, -0.5584,  0.2429, -0.3317, -0.3356,\n",
       "          -0.1391,  0.1169, -0.5680,  1.1284, -0.5904, -3.4506,  0.9443, -1.2347,\n",
       "           0.3601, -1.1216,  0.7700, -0.6213, -0.5980,  1.7820,  0.0912,  2.4645,\n",
       "           1.4746,  0.1231,  0.9806,  1.6135,  0.7534,  0.0486,  0.6767, -0.7076,\n",
       "          -0.5999, -0.0420,  1.8394, -0.4134, -0.8582, -0.8793,  2.9480, -0.9450,\n",
       "          -1.4922, -0.1948,  0.1405,  0.1409, -0.1894, -0.8533, -0.0552,  0.6463,\n",
       "          -1.0350,  0.1541,  0.6802, -0.1944, -0.0713, -1.3324, -1.1162,  0.8769,\n",
       "          -0.7394,  0.1237,  0.3905,  0.2219,  0.8339,  2.5401, -0.5636, -0.1140,\n",
       "          -1.2921,  0.2944,  3.3233, -0.4142,  2.2768, -0.0157,  0.7798, -1.1017,\n",
       "           0.1809,  0.0477,  0.7081, -2.3512,  0.8728, -1.2357, -1.2319,  1.0904,\n",
       "           0.5455,  0.1080,  1.0425, -2.3727, -0.5827,  0.1499,  1.1252,  0.0523,\n",
       "           0.7716, -0.3692,  1.0979,  0.9912, -0.7099,  1.7813, -0.2724, -1.1917,\n",
       "          -0.4040, -1.8130, -0.2113,  0.8007, -0.2540,  0.3146,  0.0452,  1.0005,\n",
       "           0.3081, -1.7167,  0.4472, -0.7984, -0.0560, -1.1856,  0.4550, -0.2208,\n",
       "          -0.6364, -0.6312, -0.0680,  0.1031, -0.1457, -0.9463, -0.3921,  1.8091,\n",
       "          -0.0359,  1.7510,  1.8990, -0.5390, -1.3628,  0.1600, -0.4786,  1.9446,\n",
       "          -1.7203, -0.5788,  0.5974, -2.4986,  1.0342,  1.4896,  2.9698, -3.3848,\n",
       "          -0.0820,  0.2793, -0.6146, -2.1241, -0.1066,  0.7602,  2.5793, -1.6181,\n",
       "          -0.3917, -0.5867, -0.7124,  0.8716,  0.6519,  0.8150, -4.0124,  0.1151,\n",
       "           0.4168, -0.9800,  1.1370, -4.6463,  0.3652, -0.2340,  0.9060,  0.3483,\n",
       "           0.3712, -0.1454, -0.0696,  0.1689, -0.1229, -0.0095, -0.8922, -1.4082,\n",
       "          -1.3216, -0.9042,  0.4494,  0.1759,  0.4837, -1.4023,  0.1709,  0.2917,\n",
       "          -0.6060, -0.3107, -2.8848, -2.7061, -0.7606, -0.4852, -1.1580,  0.8769,\n",
       "          -0.6393,  1.7344, -1.4017,  1.1119,  1.0297,  1.1121, -0.9473, -0.3119,\n",
       "           0.1751, -1.4411, -0.9983,  0.1189, -0.3039, -0.2300,  1.0746, -0.8949,\n",
       "           0.3394, -0.5449,  0.5390,  2.5465,  0.1863,  0.7803,  0.6382, -0.1651,\n",
       "           1.2941,  0.1473,  0.0655,  0.4128,  0.2674, -1.3806,  0.3477, -0.4984,\n",
       "           2.8138,  1.0120,  0.4053, -0.3879, -0.4316, -0.2375, -0.0296, -1.0811,\n",
       "           0.8780, -0.1016, -1.1959, -0.6489, -1.5777, -0.2412, -0.1582,  2.3589,\n",
       "           0.2171, -0.4483, -0.2292,  0.7171, -0.2780,  0.2822, -0.4410,  1.4184]],\n",
       "        device='cuda:0', grad_fn=<HardtanhBackward0>),\n",
       " 'meaning_hidden': tensor([[-0.0516,  0.0860,  0.0934,  0.0153, -0.0192, -0.0567, -0.1301,  0.0610,\n",
       "           0.0742,  0.0037,  0.0620, -0.0353,  0.1053, -0.0321,  0.0769, -0.0044,\n",
       "          -0.0142,  0.0558, -0.0172, -0.0675,  0.1250,  0.1142,  0.0200, -0.1156,\n",
       "          -0.1011,  0.0928, -0.0418,  0.0025, -0.0321, -0.0030, -0.1026, -0.0253,\n",
       "          -0.0403,  0.0778, -0.0874, -0.0515,  0.1547,  0.1286,  0.0127,  0.0275,\n",
       "           0.0606,  0.0189, -0.0852,  0.1140,  0.0776, -0.1291,  0.0512, -0.0957,\n",
       "          -0.0236,  0.0007, -0.0187,  0.0779, -0.1481,  0.0799,  0.1106,  0.0225,\n",
       "          -0.0030, -0.0681, -0.0010, -0.0299,  0.0467, -0.0554,  0.0299, -0.0661,\n",
       "          -0.0952, -0.0420,  0.1096, -0.0464,  0.0842,  0.0862,  0.0406, -0.0474,\n",
       "          -0.1037, -0.1390,  0.0514,  0.0335,  0.0429,  0.0875, -0.0487,  0.0358,\n",
       "           0.0114, -0.0285, -0.0324,  0.0327, -0.0417, -0.1015, -0.1088,  0.0259,\n",
       "           0.1717,  0.0808,  0.0613,  0.0395,  0.0255,  0.0357,  0.0376, -0.0779,\n",
       "           0.0272,  0.0483,  0.0076, -0.0821,  0.0535,  0.1087, -0.1090, -0.1533,\n",
       "           0.1542, -0.0389, -0.0904, -0.0310,  0.0704, -0.0795,  0.0319, -0.0615,\n",
       "          -0.1067, -0.0108, -0.0484, -0.0485, -0.0392, -0.1415,  0.1077, -0.0874,\n",
       "          -0.1687,  0.0260, -0.0673, -0.0366, -0.0439,  0.0273,  0.0657, -0.0758,\n",
       "           0.0214, -0.2290,  0.0335,  0.1520, -0.1244, -0.0164,  0.0166, -0.0898,\n",
       "          -0.0192, -0.0167,  0.1396,  0.0036,  0.0489, -0.0105,  0.0623, -0.0563,\n",
       "          -0.1159, -0.0049,  0.1376, -0.0117, -0.0698,  0.0103, -0.0724, -0.0043,\n",
       "           0.0550,  0.0137,  0.0780, -0.0945, -0.1778, -0.0645,  0.0296,  0.0408,\n",
       "          -0.0304, -0.0752, -0.0214,  0.0629,  0.0779, -0.0034,  0.0536,  0.0369,\n",
       "          -0.0129,  0.0364, -0.1115, -0.0817,  0.1959, -0.0216,  0.0793, -0.0174,\n",
       "          -0.1098,  0.0091,  0.1478, -0.0172,  0.0953,  0.0734,  0.0275, -0.0998,\n",
       "          -0.0375, -0.0870,  0.0540,  0.1358,  0.0714, -0.1303,  0.1002, -0.1094,\n",
       "           0.0346,  0.0472, -0.0448,  0.0237, -0.0013,  0.0778,  0.0856, -0.0708,\n",
       "          -0.0296, -0.0036, -0.0258, -0.0865,  0.0575, -0.0465, -0.0278,  0.0599,\n",
       "           0.0203, -0.0903, -0.0561, -0.0821,  0.0345,  0.0292, -0.0170, -0.0894,\n",
       "           0.0365, -0.0032,  0.0198,  0.0993, -0.0779,  0.1123,  0.0184,  0.0641,\n",
       "           0.0528, -0.0572,  0.0146,  0.0136,  0.0317,  0.0018, -0.0282, -0.0873,\n",
       "           0.0108,  0.0578,  0.0405, -0.0890,  0.1427, -0.0767, -0.1147, -0.0190,\n",
       "           0.0216, -0.0447,  0.1010, -0.0307, -0.0251, -0.0251, -0.0131, -0.0692,\n",
       "           0.0615, -0.0975,  0.0807, -0.0150,  0.0980, -0.0461,  0.0015,  0.0630,\n",
       "           0.0087, -0.0106, -0.1449,  0.0479, -0.0743, -0.1121,  0.0146, -0.0991,\n",
       "          -0.0521,  0.0438, -0.0732, -0.0037, -0.1289,  0.0107, -0.0222,  0.0821,\n",
       "           0.0128,  0.1488, -0.1212, -0.0267, -0.0814, -0.0082,  0.0254, -0.0929,\n",
       "           0.0390,  0.0013, -0.1201, -0.0464, -0.1009, -0.0286, -0.0318, -0.0851,\n",
       "          -0.0269,  0.0982,  0.0059, -0.0535, -0.0870,  0.0741,  0.0321, -0.0730,\n",
       "          -0.0635,  0.0802, -0.0919,  0.0080],\n",
       "         [ 0.1151, -0.1436, -0.0410, -0.0718,  0.0400, -0.2186, -0.0190, -0.1394,\n",
       "           0.1942,  0.0577,  0.0691,  0.1039,  0.0104,  0.1910,  0.2270,  0.0861,\n",
       "          -0.0114, -0.1513,  0.0916,  0.1948,  0.4662,  0.2228, -0.1103, -0.3451,\n",
       "           0.1512,  0.1191, -0.2565,  0.0299,  0.1555,  0.0458,  0.0703,  0.1774,\n",
       "           0.0896, -0.1395, -0.2851,  0.0598, -0.0616,  0.3513, -0.0849, -0.0222,\n",
       "          -0.1975,  0.2362, -0.0310,  0.2208,  0.1290, -0.0931,  0.0319,  0.0181,\n",
       "          -0.0430, -0.0250,  0.0980,  0.2651,  0.1079, -0.0962,  0.1475,  0.0539,\n",
       "          -0.0270,  0.0803, -0.0017,  0.0375, -0.0398,  0.0814,  0.0573,  0.1632,\n",
       "          -0.0345,  0.0251, -0.1820, -0.0183,  0.3719, -0.1480, -0.0779,  0.0188,\n",
       "          -0.1145, -0.1644,  0.0566,  0.1003,  0.1201, -0.1276,  0.0022,  0.0692,\n",
       "          -0.0412,  0.1950,  0.1714,  0.1448, -0.2862, -0.1194, -0.2141,  0.0844,\n",
       "           0.1872,  0.0271,  0.2977, -0.0764,  0.0421, -0.0412, -0.1529, -0.0319,\n",
       "           0.0711, -0.0395, -0.2343, -0.1428, -0.1343,  0.0877,  0.1455, -0.0936,\n",
       "           0.3148,  0.1319, -0.0800,  0.0253,  0.0885, -0.2250,  0.0694, -0.1256,\n",
       "           0.0276, -0.0067,  0.1152,  0.1032,  0.0637,  0.0132,  0.2263, -0.1889,\n",
       "          -0.4154, -0.0859, -0.1523, -0.3218,  0.0049, -0.0232,  0.0511, -0.4241,\n",
       "           0.1698, -0.3027, -0.0934,  0.2633, -0.2212, -0.1932,  0.0732, -0.3449,\n",
       "           0.1197, -0.0589,  0.1980, -0.1015, -0.1630, -0.0394,  0.1914, -0.1131,\n",
       "           0.2009,  0.0579,  0.2413,  0.0596, -0.0111, -0.1301,  0.0292, -0.0254,\n",
       "           0.0436,  0.0145,  0.1325, -0.1262,  0.0574, -0.0176, -0.0179,  0.0020,\n",
       "          -0.0155, -0.3665,  0.3167,  0.2688,  0.0870,  0.0044,  0.0884, -0.0892,\n",
       "           0.1442,  0.0827, -0.2114, -0.1628,  0.2485,  0.0817,  0.3258, -0.0686,\n",
       "          -0.1846,  0.0728,  0.1662, -0.1396, -0.1943,  0.0055, -0.1358, -0.2411,\n",
       "           0.1114,  0.0752,  0.0829,  0.2107, -0.0715,  0.0901,  0.1321, -0.0861,\n",
       "          -0.0519, -0.1624, -0.0544,  0.1436, -0.0513, -0.2129,  0.3148, -0.1656,\n",
       "          -0.0580,  0.0552,  0.0243,  0.1360, -0.1599, -0.0190,  0.0658,  0.0222,\n",
       "          -0.0693,  0.0464,  0.0797, -0.1189, -0.0975,  0.0975,  0.0453, -0.1217,\n",
       "          -0.0984, -0.0645,  0.1223, -0.1567, -0.0131, -0.2396,  0.0873,  0.1945,\n",
       "           0.1051,  0.0346, -0.1674,  0.0158, -0.0825, -0.1227, -0.0694, -0.0438,\n",
       "           0.0710,  0.1179, -0.0232,  0.1237,  0.4053, -0.0915, -0.3744, -0.0320,\n",
       "          -0.0217,  0.0069,  0.2231,  0.1825, -0.1250, -0.0793, -0.0932, -0.0094,\n",
       "           0.0143, -0.3653,  0.0006,  0.0330, -0.1502, -0.0571, -0.0776,  0.0943,\n",
       "          -0.1513,  0.0941, -0.3251, -0.1753, -0.0692, -0.4036, -0.1895, -0.0601,\n",
       "           0.1946, -0.0064, -0.2049, -0.1701, -0.3300, -0.0592, -0.0075,  0.0953,\n",
       "          -0.0572, -0.1107, -0.0669, -0.1304, -0.0641,  0.0580,  0.2781,  0.1449,\n",
       "          -0.2054,  0.3128, -0.3238,  0.1125,  0.3059,  0.0229, -0.0823, -0.1176,\n",
       "           0.0667,  0.0102,  0.0439,  0.1207,  0.0945,  0.0069,  0.0595,  0.0351,\n",
       "          -0.0267,  0.0960, -0.0864,  0.0168]], device='cuda:0',\n",
       "        grad_fn=<HardtanhBackward0>),\n",
       " 'style_hidden': tensor([[ 9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "           9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01],\n",
       "         [-1.8673e-01, -6.8383e-03, -1.0972e-02, -4.1504e-02,  4.5450e-02,\n",
       "           4.2761e-02,  3.2124e-02, -7.7165e-03,  2.6503e-01,  9.2345e-02,\n",
       "          -2.7182e-01,  3.4910e-02,  1.9602e-01, -1.4242e-01,  1.6818e-01,\n",
       "          -1.2083e-01, -1.1202e-01, -1.1750e-01,  4.2902e-03,  9.2927e-02,\n",
       "          -1.1483e-01,  4.1601e-02, -4.2411e-01,  9.0947e-02, -8.4838e-03,\n",
       "          -1.2627e-01,  4.1469e-01,  2.5599e-01,  4.3508e-01,  3.9776e-01,\n",
       "          -1.7019e-01, -6.1436e-02,  1.7236e-01, -2.0614e-02, -1.4754e-03,\n",
       "          -1.3432e-01, -2.9641e-02,  5.2394e-04, -2.9039e-03, -1.2524e-01,\n",
       "           8.1127e-02, -2.3381e-02,  1.8228e-01,  1.4778e-01,  6.8029e-02,\n",
       "           6.5773e-02,  2.3562e-01, -2.4931e-01, -1.6593e-01,  2.1291e-01,\n",
       "          -1.6395e-02, -5.2244e-02,  4.8224e-02, -1.0983e-01,  1.9703e-01,\n",
       "           9.2984e-03, -1.9185e-01,  2.6237e-02,  2.2996e-01,  1.1089e-02,\n",
       "          -1.9774e-01,  1.4980e-01, -1.1461e-01,  4.8494e-02,  8.5271e-02,\n",
       "           1.0738e-01, -2.4062e-01,  3.7794e-01, -1.8434e-01,  1.1110e-02,\n",
       "          -7.5055e-02, -4.6177e-02,  9.6815e-02,  5.3441e-02,  7.1436e-02,\n",
       "           1.3487e-01,  2.0392e-01,  9.0846e-02, -1.6120e-01,  3.4361e-01,\n",
       "           1.7270e-01,  4.0415e-02, -1.2106e-01,  1.8246e-01,  3.1045e-01,\n",
       "          -1.7908e-01,  1.9844e-01, -5.1075e-02,  4.1835e-02, -3.5073e-02,\n",
       "           2.3618e-01,  1.5601e-01, -1.3490e-01, -1.9824e-01, -2.1633e-01,\n",
       "          -6.2465e-03,  7.1184e-02, -8.3875e-02,  8.2747e-02, -2.8336e-01,\n",
       "           4.4249e-03,  2.5649e-01, -1.2644e-01,  9.1683e-02,  2.8344e-02,\n",
       "           1.1490e-01, -1.7818e-01,  1.6401e-01, -1.8352e-01, -1.3104e-01,\n",
       "          -1.9652e-02, -1.2347e-01,  3.1577e-02,  1.6262e-01, -2.5529e-01,\n",
       "          -7.3971e-02, -4.9989e-02,  6.4862e-02,  8.0388e-02,  3.2430e-03,\n",
       "           2.8856e-02,  9.4640e-02,  1.0347e-01, -1.1913e-01, -2.6492e-01,\n",
       "          -1.7301e-01, -1.4588e-01,  2.0005e-01, -2.3572e-01, -9.4336e-02,\n",
       "          -2.0929e-01, -1.0738e-01, -1.7838e-01, -2.1031e-02,  5.3319e-02,\n",
       "          -1.3118e-01, -2.5148e-02,  8.0844e-02, -7.0334e-02, -4.3653e-02,\n",
       "          -6.4491e-02,  8.6427e-02, -7.4982e-02,  7.3839e-02, -1.3995e-01,\n",
       "           8.5040e-03, -4.6480e-02, -4.0477e-02, -2.7086e-01,  2.3726e-01,\n",
       "           5.6514e-02, -4.7246e-03, -1.3618e-01, -8.9273e-02,  9.1502e-02,\n",
       "           1.0172e-01, -1.7634e-01,  1.9439e-02, -6.6545e-02, -6.1542e-02,\n",
       "          -8.5989e-02,  9.8294e-02,  7.2231e-02,  6.6020e-02,  5.9324e-02,\n",
       "           1.7823e-02,  1.1983e-01,  9.3928e-02,  6.9457e-02, -7.3473e-02,\n",
       "          -4.2236e-02, -8.6873e-02,  1.2993e-01,  4.4731e-02, -1.6413e-01,\n",
       "           3.1083e-02,  1.4668e-01,  4.6888e-02,  2.7144e-02,  1.5134e-01,\n",
       "          -1.4226e-01, -2.4197e-01, -2.7270e-02, -1.4963e-01,  1.3354e-01,\n",
       "           1.4415e-01, -1.7468e-01,  8.7353e-02, -2.5465e-01, -5.1361e-03,\n",
       "          -1.7966e-01,  1.1294e-01,  5.1760e-03,  2.5555e-02,  2.0901e-01,\n",
       "           7.8570e-02,  1.0488e-01, -7.2134e-03,  3.3182e-02, -1.2506e-01,\n",
       "          -9.5549e-02, -1.1879e-01,  1.3411e-02, -2.6338e-01,  6.9911e-02,\n",
       "          -1.4768e-02,  1.3346e-01,  1.3946e-01,  4.8535e-02, -3.5046e-01,\n",
       "          -1.1077e-01,  1.4009e-01,  1.7218e-01,  8.2825e-02,  1.1712e-01,\n",
       "           1.0298e-01, -4.7667e-02,  4.0856e-02,  4.6981e-02,  4.7390e-03,\n",
       "          -1.0780e-01,  2.0259e-01, -2.3183e-01, -2.8457e-01,  7.7863e-02,\n",
       "          -5.2006e-02,  2.3247e-03,  1.9698e-01,  4.7268e-02, -2.2711e-02,\n",
       "           4.7397e-02, -3.7185e-01,  1.8514e-01, -1.8245e-01, -2.2073e-01,\n",
       "          -5.9303e-02,  2.4863e-01, -6.0634e-02, -1.8683e-01,  2.7572e-01,\n",
       "           9.0144e-02,  4.2072e-01, -1.1475e-01,  1.3060e-01, -2.6675e-02,\n",
       "          -1.0482e-02, -1.8165e-01, -2.3880e-01,  7.9341e-02, -1.6747e-01,\n",
       "          -3.0622e-02,  1.6844e-01,  4.1378e-02,  9.4616e-02,  9.4553e-02,\n",
       "           2.8878e-01,  4.3966e-03,  1.3216e-03,  4.5799e-02,  6.7966e-02,\n",
       "           3.5462e-01,  1.2977e-01,  1.5001e-01,  2.1213e-02,  2.5707e-01,\n",
       "           2.8019e-01,  2.6373e-01, -1.0863e-01, -2.2194e-02,  5.1562e-02,\n",
       "           1.6521e-01, -2.8015e-01, -4.0440e-02,  1.9637e-01,  5.6778e-02,\n",
       "          -2.7575e-01,  8.4954e-03, -1.1558e-01,  3.1590e-01,  4.5091e-03,\n",
       "           1.9798e-01,  5.3916e-02,  2.5428e-01, -9.0749e-03,  8.9994e-02,\n",
       "           3.5863e-01,  2.1162e-01, -1.5985e-01, -8.0782e-02,  1.7633e-01,\n",
       "           1.5625e-01, -1.7251e-01, -2.1503e-01, -6.1276e-02,  7.3790e-02,\n",
       "           1.3437e-01,  9.3630e-02,  1.9630e-01, -5.5855e-03,  3.0035e-01]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " 'logits': tensor([[[-12.5397, -12.5452,  -4.2268,  ...,  -8.6519, -12.0083,  -8.2720],\n",
       "          [ -9.0538,  -9.0128,  -5.3591,  ...,  -5.5835,  -8.7017,  -8.6701],\n",
       "          [-12.7682, -12.7778,  -5.8234,  ..., -11.6527, -13.8091, -12.6297],\n",
       "          ...,\n",
       "          [-11.0016, -11.1065,  10.0830,  ..., -14.0557, -14.8827, -11.4681],\n",
       "          [-10.6382, -10.7388,  10.0829,  ..., -14.0935, -14.1978, -11.4484],\n",
       "          [-10.3292, -10.4278,   9.9092,  ..., -14.1629, -13.7166, -11.4860]],\n",
       " \n",
       "         [[ -8.8980,  -8.7850,  -5.1215,  ...,  -0.7170,  -5.3179,  -2.2933],\n",
       "          [-11.8533, -11.8790,  -5.9339,  ...,  -7.1608,  -9.7720,  -5.6497],\n",
       "          [-12.1186, -12.1288,  -3.9980,  ...,  -7.2657, -13.3467,  -6.1262],\n",
       "          ...,\n",
       "          [ -6.8319,  -6.8326,  11.1633,  ...,  -8.7736,  -9.0207,  -6.7346],\n",
       "          [ -6.6418,  -6.6348,  11.1725,  ...,  -8.6960,  -8.7375,  -6.7755],\n",
       "          [ -6.5164,  -6.5021,  11.0901,  ...,  -8.6483,  -8.6068,  -6.8891]]],\n",
       "        device='cuda:0', grad_fn=<CatBackward>),\n",
       " 'predictions': tensor([[  12, 1313,    5,  454,  689,  216,    8,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2],\n",
       "         [ 281,   16, 1042,  171,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2]], device='cuda:0'),\n",
       " 'loss': tensor(0.8584, device='cuda:0', grad_fn=<NllLossBackward>)}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentences = get_sentences(original_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rice was hard things that .\n",
      "which is awesome !\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(original_sentences[0]))\n",
    "print(' '.join(original_sentences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model(create_inputs('sophistication extraordinary asdsadasdadsadasdsadasdsadasdadsadsd <unk>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['great', 'for', 'a', 'good', 'food', 'at', 'what', '!']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentences(model.decode(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1151, -0.1436, -0.0410, -0.0718,  0.0400, -0.2186, -0.0190, -0.1394,\n",
       "         0.1942,  0.0577,  0.0691,  0.1039,  0.0104,  0.1910,  0.2270,  0.0861,\n",
       "        -0.0114, -0.1513,  0.0916,  0.1948,  0.4662,  0.2228, -0.1103, -0.3451,\n",
       "         0.1512,  0.1191, -0.2565,  0.0299,  0.1555,  0.0458,  0.0703,  0.1774,\n",
       "         0.0896, -0.1395, -0.2851,  0.0598, -0.0616,  0.3513, -0.0849, -0.0222,\n",
       "        -0.1975,  0.2362, -0.0310,  0.2208,  0.1290, -0.0931,  0.0319,  0.0181,\n",
       "        -0.0430, -0.0250,  0.0980,  0.2651,  0.1079, -0.0962,  0.1475,  0.0539,\n",
       "        -0.0270,  0.0803, -0.0017,  0.0375, -0.0398,  0.0814,  0.0573,  0.1632,\n",
       "        -0.0345,  0.0251, -0.1820, -0.0183,  0.3719, -0.1480, -0.0779,  0.0188,\n",
       "        -0.1145, -0.1644,  0.0566,  0.1003,  0.1201, -0.1276,  0.0022,  0.0692,\n",
       "        -0.0412,  0.1950,  0.1714,  0.1448, -0.2862, -0.1194, -0.2141,  0.0844,\n",
       "         0.1872,  0.0271,  0.2977, -0.0764,  0.0421, -0.0412, -0.1529, -0.0319,\n",
       "         0.0711, -0.0395, -0.2343, -0.1428, -0.1343,  0.0877,  0.1455, -0.0936,\n",
       "         0.3148,  0.1319, -0.0800,  0.0253,  0.0885, -0.2250,  0.0694, -0.1256,\n",
       "         0.0276, -0.0067,  0.1152,  0.1032,  0.0637,  0.0132,  0.2263, -0.1889,\n",
       "        -0.4154, -0.0859, -0.1523, -0.3218,  0.0049, -0.0232,  0.0511, -0.4241,\n",
       "         0.1698, -0.3027, -0.0934,  0.2633, -0.2212, -0.1932,  0.0732, -0.3449,\n",
       "         0.1197, -0.0589,  0.1980, -0.1015, -0.1630, -0.0394,  0.1914, -0.1131,\n",
       "         0.2009,  0.0579,  0.2413,  0.0596, -0.0111, -0.1301,  0.0292, -0.0254,\n",
       "         0.0436,  0.0145,  0.1325, -0.1262,  0.0574, -0.0176, -0.0179,  0.0020,\n",
       "        -0.0155, -0.3665,  0.3167,  0.2688,  0.0870,  0.0044,  0.0884, -0.0892,\n",
       "         0.1442,  0.0827, -0.2114, -0.1628,  0.2485,  0.0817,  0.3258, -0.0686,\n",
       "        -0.1846,  0.0728,  0.1662, -0.1396, -0.1943,  0.0055, -0.1358, -0.2411,\n",
       "         0.1114,  0.0752,  0.0829,  0.2107, -0.0715,  0.0901,  0.1321, -0.0861,\n",
       "        -0.0519, -0.1624, -0.0544,  0.1436, -0.0513, -0.2129,  0.3148, -0.1656,\n",
       "        -0.0580,  0.0552,  0.0243,  0.1360, -0.1599, -0.0190,  0.0658,  0.0222,\n",
       "        -0.0693,  0.0464,  0.0797, -0.1189, -0.0975,  0.0975,  0.0453, -0.1217,\n",
       "        -0.0984, -0.0645,  0.1223, -0.1567, -0.0131, -0.2396,  0.0873,  0.1945,\n",
       "         0.1051,  0.0346, -0.1674,  0.0158, -0.0825, -0.1227, -0.0694, -0.0438,\n",
       "         0.0710,  0.1179, -0.0232,  0.1237,  0.4053, -0.0915, -0.3744, -0.0320,\n",
       "        -0.0217,  0.0069,  0.2231,  0.1825, -0.1250, -0.0793, -0.0932, -0.0094,\n",
       "         0.0143, -0.3653,  0.0006,  0.0330, -0.1502, -0.0571, -0.0776,  0.0943,\n",
       "        -0.1513,  0.0941, -0.3251, -0.1753, -0.0692, -0.4036, -0.1895, -0.0601,\n",
       "         0.1946, -0.0064, -0.2049, -0.1701, -0.3300, -0.0592, -0.0075,  0.0953,\n",
       "        -0.0572, -0.1107, -0.0669, -0.1304, -0.0641,  0.0580,  0.2781,  0.1449,\n",
       "        -0.2054,  0.3128, -0.3238,  0.1125,  0.3059,  0.0229, -0.0823, -0.1176,\n",
       "         0.0667,  0.0102,  0.0439,  0.1207,  0.0945,  0.0069,  0.0595,  0.0351,\n",
       "        -0.0267,  0.0960, -0.0864,  0.0168], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_hidden['meaning_hidden'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, z_hidden['style_hidden'][0].shape[0]):\n",
    "    z_hidden['style_hidden'][0][i] = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_hidden['style_hidden'][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,\n",
       "          9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01,  9.0000e-01],\n",
       "        [-1.8673e-01, -6.8383e-03, -1.0972e-02, -4.1504e-02,  4.5450e-02,\n",
       "          4.2761e-02,  3.2124e-02, -7.7165e-03,  2.6503e-01,  9.2345e-02,\n",
       "         -2.7182e-01,  3.4910e-02,  1.9602e-01, -1.4242e-01,  1.6818e-01,\n",
       "         -1.2083e-01, -1.1202e-01, -1.1750e-01,  4.2902e-03,  9.2927e-02,\n",
       "         -1.1483e-01,  4.1601e-02, -4.2411e-01,  9.0947e-02, -8.4838e-03,\n",
       "         -1.2627e-01,  4.1469e-01,  2.5599e-01,  4.3508e-01,  3.9776e-01,\n",
       "         -1.7019e-01, -6.1436e-02,  1.7236e-01, -2.0614e-02, -1.4754e-03,\n",
       "         -1.3432e-01, -2.9641e-02,  5.2394e-04, -2.9039e-03, -1.2524e-01,\n",
       "          8.1127e-02, -2.3381e-02,  1.8228e-01,  1.4778e-01,  6.8029e-02,\n",
       "          6.5773e-02,  2.3562e-01, -2.4931e-01, -1.6593e-01,  2.1291e-01,\n",
       "         -1.6395e-02, -5.2244e-02,  4.8224e-02, -1.0983e-01,  1.9703e-01,\n",
       "          9.2984e-03, -1.9185e-01,  2.6237e-02,  2.2996e-01,  1.1089e-02,\n",
       "         -1.9774e-01,  1.4980e-01, -1.1461e-01,  4.8494e-02,  8.5271e-02,\n",
       "          1.0738e-01, -2.4062e-01,  3.7794e-01, -1.8434e-01,  1.1110e-02,\n",
       "         -7.5055e-02, -4.6177e-02,  9.6815e-02,  5.3441e-02,  7.1436e-02,\n",
       "          1.3487e-01,  2.0392e-01,  9.0846e-02, -1.6120e-01,  3.4361e-01,\n",
       "          1.7270e-01,  4.0415e-02, -1.2106e-01,  1.8246e-01,  3.1045e-01,\n",
       "         -1.7908e-01,  1.9844e-01, -5.1075e-02,  4.1835e-02, -3.5073e-02,\n",
       "          2.3618e-01,  1.5601e-01, -1.3490e-01, -1.9824e-01, -2.1633e-01,\n",
       "         -6.2465e-03,  7.1184e-02, -8.3875e-02,  8.2747e-02, -2.8336e-01,\n",
       "          4.4249e-03,  2.5649e-01, -1.2644e-01,  9.1683e-02,  2.8344e-02,\n",
       "          1.1490e-01, -1.7818e-01,  1.6401e-01, -1.8352e-01, -1.3104e-01,\n",
       "         -1.9652e-02, -1.2347e-01,  3.1577e-02,  1.6262e-01, -2.5529e-01,\n",
       "         -7.3971e-02, -4.9989e-02,  6.4862e-02,  8.0388e-02,  3.2430e-03,\n",
       "          2.8856e-02,  9.4640e-02,  1.0347e-01, -1.1913e-01, -2.6492e-01,\n",
       "         -1.7301e-01, -1.4588e-01,  2.0005e-01, -2.3572e-01, -9.4336e-02,\n",
       "         -2.0929e-01, -1.0738e-01, -1.7838e-01, -2.1031e-02,  5.3319e-02,\n",
       "         -1.3118e-01, -2.5148e-02,  8.0844e-02, -7.0334e-02, -4.3653e-02,\n",
       "         -6.4491e-02,  8.6427e-02, -7.4982e-02,  7.3839e-02, -1.3995e-01,\n",
       "          8.5040e-03, -4.6480e-02, -4.0477e-02, -2.7086e-01,  2.3726e-01,\n",
       "          5.6514e-02, -4.7246e-03, -1.3618e-01, -8.9273e-02,  9.1502e-02,\n",
       "          1.0172e-01, -1.7634e-01,  1.9439e-02, -6.6545e-02, -6.1542e-02,\n",
       "         -8.5989e-02,  9.8294e-02,  7.2231e-02,  6.6020e-02,  5.9324e-02,\n",
       "          1.7823e-02,  1.1983e-01,  9.3928e-02,  6.9457e-02, -7.3473e-02,\n",
       "         -4.2236e-02, -8.6873e-02,  1.2993e-01,  4.4731e-02, -1.6413e-01,\n",
       "          3.1083e-02,  1.4668e-01,  4.6888e-02,  2.7144e-02,  1.5134e-01,\n",
       "         -1.4226e-01, -2.4197e-01, -2.7270e-02, -1.4963e-01,  1.3354e-01,\n",
       "          1.4415e-01, -1.7468e-01,  8.7353e-02, -2.5465e-01, -5.1361e-03,\n",
       "         -1.7966e-01,  1.1294e-01,  5.1760e-03,  2.5555e-02,  2.0901e-01,\n",
       "          7.8570e-02,  1.0488e-01, -7.2134e-03,  3.3182e-02, -1.2506e-01,\n",
       "         -9.5549e-02, -1.1879e-01,  1.3411e-02, -2.6338e-01,  6.9911e-02,\n",
       "         -1.4768e-02,  1.3346e-01,  1.3946e-01,  4.8535e-02, -3.5046e-01,\n",
       "         -1.1077e-01,  1.4009e-01,  1.7218e-01,  8.2825e-02,  1.1712e-01,\n",
       "          1.0298e-01, -4.7667e-02,  4.0856e-02,  4.6981e-02,  4.7390e-03,\n",
       "         -1.0780e-01,  2.0259e-01, -2.3183e-01, -2.8457e-01,  7.7863e-02,\n",
       "         -5.2006e-02,  2.3247e-03,  1.9698e-01,  4.7268e-02, -2.2711e-02,\n",
       "          4.7397e-02, -3.7185e-01,  1.8514e-01, -1.8245e-01, -2.2073e-01,\n",
       "         -5.9303e-02,  2.4863e-01, -6.0634e-02, -1.8683e-01,  2.7572e-01,\n",
       "          9.0144e-02,  4.2072e-01, -1.1475e-01,  1.3060e-01, -2.6675e-02,\n",
       "         -1.0482e-02, -1.8165e-01, -2.3880e-01,  7.9341e-02, -1.6747e-01,\n",
       "         -3.0622e-02,  1.6844e-01,  4.1378e-02,  9.4616e-02,  9.4553e-02,\n",
       "          2.8878e-01,  4.3966e-03,  1.3216e-03,  4.5799e-02,  6.7966e-02,\n",
       "          3.5462e-01,  1.2977e-01,  1.5001e-01,  2.1213e-02,  2.5707e-01,\n",
       "          2.8019e-01,  2.6373e-01, -1.0863e-01, -2.2194e-02,  5.1562e-02,\n",
       "          1.6521e-01, -2.8015e-01, -4.0440e-02,  1.9637e-01,  5.6778e-02,\n",
       "         -2.7575e-01,  8.4954e-03, -1.1558e-01,  3.1590e-01,  4.5091e-03,\n",
       "          1.9798e-01,  5.3916e-02,  2.5428e-01, -9.0749e-03,  8.9994e-02,\n",
       "          3.5863e-01,  2.1162e-01, -1.5985e-01, -8.0782e-02,  1.7633e-01,\n",
       "          1.5625e-01, -1.7251e-01, -2.1503e-01, -6.1276e-02,  7.3790e-02,\n",
       "          1.3437e-01,  9.3630e-02,  1.9630e-01, -5.5855e-03,  3.0035e-01]],\n",
       "       device='cuda:0', grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_hidden['style_hidden']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
       "        0.9000, 0.9000, 0.9000], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_hidden['style_hidden'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hidden_swapped = {\n",
    "    'meaning_hidden': torch.stack([\n",
    "        z_hidden['meaning_hidden'][0].clone(),\n",
    "        #z_hidden['meaning_hidden'][1].clone(),        \n",
    "    ], dim=0),\n",
    "    'style_hidden': torch.stack([\n",
    "        z_hidden['style_hidden'][0].clone(),\n",
    "        #z_hidden['style_hidden'][0].clone(),        \n",
    "    ], dim=0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaped_decoded = model.decode(z_hidden_swapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaped_sentences = get_sentences(swaped_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rice was hard things that .\n",
      "\n",
      "fish fresh and were delicious !\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(original_sentences[0]))\n",
    "# print(' '.join(original_sentences[1]))\n",
    "print()\n",
    "print(' '.join(swaped_sentences[0]))\n",
    "# print(' '.join(swaped_sentences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVERYTHING USEFUL IS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original vocab 41574; pruned to 11004\n",
      "Number of sentences dropped from ./data/classifier/train.txt: 448221 out of 549367 total\n",
      "original vocab 41574; pruned to 11004\n",
      "Number of sentences dropped from ./data/classifier/test.txt: 8288 out of 9824 total\n"
     ]
    }
   ],
   "source": [
    "#python3.6 train_surrogate.py --data_path ./data/classifier --save_path game_output/ --classifier_path ./data --load_pretrained .\n",
    "cur_dir = '.'\n",
    "\n",
    "with open(cur_dir + '/vocab.json', 'r') as fin:\n",
    "    corpus_vocab = json.load(fin)\n",
    "\n",
    "corpus_train = SNLIDataset(train=True, vocab_size=11004-4, path='./data/classifier')\n",
    "corpus_test = SNLIDataset(train=False, vocab_size=11004-4, path='./data/classifier')\n",
    "trainloader= torch.utils.data.DataLoader(corpus_train, batch_size = 32, collate_fn=collate_snli, shuffle=True)\n",
    "train_iter = iter(trainloader)\n",
    "testloader= torch.utils.data.DataLoader(corpus_test, batch_size = 32, collate_fn=collate_snli, shuffle=False)\n",
    "random.seed(1111)\n",
    "np.random.seed(1111)\n",
    "torch.manual_seed(1111)\n",
    "\n",
    "EPS = 3e-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset', 'batch_size', 'num_workers', 'collate_fn', 'pin_memory', 'drop_last', 'timeout', 'worker_init_fn', 'sampler', 'batch_sampler', '_DataLoader__initialized'])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.SNLIDataset at 0x7fac83c55a58>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/game/.local/lib/python3.6/site-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'models.Seq2SeqCAE' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline_Embeddings(\n",
      "  (embedding_prem): Embedding(11004, 100)\n",
      "  (embedding_hypo): Embedding(11004, 100)\n",
      "  (linear): Linear(in_features=200, out_features=3, bias=True)\n",
      ")\n",
      "Seq2SeqCAE(\n",
      "  (embedding): Embedding(11004, 300)\n",
      "  (embedding_decoder): Embedding(11004, 300)\n",
      "  (encoder): Sequential(\n",
      "    (layer-1): Conv1d(300, 500, kernel_size=(3,), stride=(1,))\n",
      "    (bn-1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation-1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (layer-2): Conv1d(500, 700, kernel_size=(3,), stride=(2,))\n",
      "    (bn-2): BatchNorm1d(700, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation-2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (layer-3): Conv1d(700, 1000, kernel_size=(3,), stride=(2,))\n",
      "    (bn-3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation-3): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (linear): Linear(in_features=1000, out_features=300, bias=True)\n",
      "  (decoder): LSTM(600, 300, batch_first=True)\n",
      "  (linear_dec): Linear(in_features=300, out_features=11004, bias=True)\n",
      ")\n",
      "MLP_I(\n",
      "  (layer1): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (bn1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation1): ReLU()\n",
      "  (layer2): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): ReLU()\n",
      "  (layer7): Linear(in_features=300, out_features=100, bias=True)\n",
      ")\n",
      "MLPClassifier(\n",
      "  (layers): Sequential(\n",
      "    (layer0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (bn0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation0): ReLU()\n",
      "    (layer1): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation1): ReLU()\n",
      "  )\n",
      "  (linear): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "autoencoder = torch.load(open(cur_dir + '/models/autoencoder_model.pt', 'rb'))\n",
    "#gan_gen = torch.load(open(cur_dir + '/models/gan_gen_model.pt', 'rb'))\n",
    "#gan_disc = torch.load(open(cur_dir + '/models/gan_disc_model.pt', 'rb'))\n",
    "inverter = torch.load(open(cur_dir + '/models/inverter_model.pt', 'rb'))\n",
    "\n",
    "classifier1 = Baseline_Embeddings(100, vocab_size=11004)\n",
    "#classifier1 = Baseline_LSTM(100,300,maxlen=args.maxlen, gpu=args.cuda)\n",
    "classifier1.load_state_dict(torch.load('./models' + \"/baseline/model_emb.pt\"))\n",
    "vocab_classifier1 = pkl.load(open('./models' + \"/vocab.pkl\", 'rb'))\n",
    "\n",
    "mlp_classifier = MLPClassifier(100 * 2, 3, layers='100-50')\n",
    "#if not args.train_mode:\n",
    "mlp_classifier.load_state_dict(torch.load('./game_output'+'/surrogate{0}.pt'.format('100-50')))\n",
    "\n",
    "print(classifier1)\n",
    "print(autoencoder)\n",
    "print(inverter)\n",
    "print(mlp_classifier)\n",
    "\n",
    "optimizer = optim.Adam(mlp_classifier.parameters(),\n",
    "                           lr=1e03,\n",
    "                           betas=(0.9, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def evaluate_model():\n",
    "    classifier1.eval()\n",
    "\n",
    "    test_iter = iter(trainloader)\n",
    "    correct=0\n",
    "    total=0\n",
    "    for batch in test_iter:\n",
    "        premise, hypothesis, target, _, _, _, _ = batch\n",
    "\n",
    "        if args.cuda:\n",
    "            premise=premise.cuda()\n",
    "            hypothesis = hypothesis.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        prob_distrib = classifier1.forward((premise, hypothesis))\n",
    "        predictions = np.argmax(prob_distrib.data.cpu().numpy(), 1)\n",
    "        correct+=len(np.where(target.data.cpu().numpy()==predictions)[0])\n",
    "        total+=premise.size(0)\n",
    "    acc=correct/float(total)\n",
    "    print(\"Accuracy:{0}\".format(acc))\n",
    "    return acc\n",
    "\n",
    "autoencoder.gpu = True\n",
    "autoencoder = autoencoder.cuda()\n",
    "autoencoder.start_symbols = autoencoder.start_symbols.cuda()\n",
    "#gan_gen = gan_gen.cuda()\n",
    "#gan_disc = gan_disc.cuda()\n",
    "classifier1 = classifier1.cuda()\n",
    "inverter = inverter.cuda()\n",
    "mlp_classifier = mlp_classifier.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_process(premise, hypothesis, target, premise_words, hypothesis_words, premise_length, hypothesis_length):\n",
    "    #mx = target.max().item()\n",
    "    #assert(mx >= 0 and mx < 3)\n",
    "    #for s, s_w in zip(premise, premise_words):\n",
    "    #    for i, w in zip(s, s_w):\n",
    "    #        assert(corpus_vocab.get(w, 3) == i)\n",
    "    #print(hypothesis_words, flush=True)\n",
    "    autoencoder.eval()\n",
    "    inverter.eval()\n",
    "    classifier1.eval()\n",
    "    mlp_classifier.train()\n",
    "\n",
    "    #print(premise.max().item(), flush=True)\n",
    "    #print(hypothesis.max().item(), flush=True)\n",
    "\n",
    "    premise_idx = torch.tensor([[corpus_vocab.get(w, 3) for w in s] for s in premise_words]).cuda()\n",
    "    hypothesis_idx = torch.tensor([[corpus_vocab.get(w, 3) for w in s] for s in hypothesis_words]).cuda()\n",
    "\n",
    "    c_prem = autoencoder.encode(premise_idx, premise_length, noise=False)\n",
    "    z_prem = inverter(c_prem).detach()\n",
    "\n",
    "    c_hypo = autoencoder.encode(hypothesis_idx, hypothesis_length, noise=False)\n",
    "    z_hypo = inverter(c_hypo).detach()\n",
    "\n",
    "    # z_comb = nn.cat((z_prem, z_hypo), 0).detach()\n",
    "\n",
    "    output = mlp_classifier(z_prem, z_hypo)\n",
    "    gold = classifier1((premise, hypothesis)).detach()\n",
    "\n",
    "    #print(output.shape, flush=True)\n",
    "    #print(gold.shape, flush=True)\n",
    "\n",
    "    acc = (torch.argmax(gold, 1) == target).to(torch.float32).mean().item()\n",
    "    acc_surrogate = (torch.argmax(output, 1) == target).to(torch.float32).mean().item()\n",
    "\n",
    "\n",
    "    loss = -torch.mean(torch.sum(output * F.softmax(gold, dim=1), 1), 0)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), acc, acc_surrogate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(criterion, premise, hypothesis, target, premise_words, hypothesis_words, premise_length, hypothesis_length):\n",
    "    autoencoder.eval()\n",
    "    inverter.eval()\n",
    "    classifier1.eval()\n",
    "    mlp_classifier.eval()\n",
    "    model.eval()\n",
    "    \n",
    "    premise_words = [premise_words]\n",
    "    hypothesis_words = [hypothesis_words]\n",
    "    premise_length = [premise_length]\n",
    "    hypothesis_length = [hypothesis_length]\n",
    "\n",
    "\n",
    "    premise_idx = torch.tensor([[corpus_vocab.get(w, 3) for w in s] for s in premise_words]).cuda()\n",
    "    hypothesis_idx = torch.tensor([[corpus_vocab.get(w, 3) for w in s] for s in hypothesis_words]).cuda()\n",
    "#     print('premise_idx = ' + str(premise_idx))\n",
    "#     print('hypothesis-idx = ' + str(hypothesis_idx))\n",
    "#     c_prem = autoencoder.encode(premise_idx, premise_length, noise=False)\n",
    "#     print(\"OLD C_PREM\")\n",
    "#     print(c_prem.shape)\n",
    "#     print(premise_words[0])\n",
    "    c_prem_full = model(create_inputs(' '.join(premise_words[0])))\n",
    "    c_prem = c_prem_full['style_hidden']\n",
    "#     print(\"C_PREM\")\n",
    "#     print(c_prem)\n",
    "#     print(\"----\")\n",
    "#     c_prem = create_inputs(' '.join(premise_words[0]))\n",
    "    z_prem = inverter(c_prem).detach()\n",
    "#     print('c_prem = ' + str(c_prem))\n",
    "#     print('z_prem = ' + str(z_prem))\n",
    "#     c_hypo = autoencoder.encode(hypothesis_idx, hypothesis_length, noise=False).detach()\n",
    "    c_hypo_full = model(create_inputs(' '.join(hypothesis_words[0])))\n",
    "    c_hypo = c_hypo_full['style_hidden'].detach()\n",
    "#     print(\"C_HYPO\")\n",
    "#     print(c_hypo)\n",
    "#     print(\"------\")\n",
    "    c_hypo.requires_grad = True\n",
    "    z_hypo = inverter(c_hypo)\n",
    "\n",
    "\n",
    "    premise = premise.unsqueeze(0)\n",
    "    hypothesis = hypothesis.unsqueeze(0)\n",
    "    target = target.unsqueeze(0)\n",
    "\n",
    "    output = mlp_classifier(z_prem, z_hypo)\n",
    "#     print(\"OUTPUT\")\n",
    "#     print(output)\n",
    "\n",
    "    loss = criterion(output, target)\n",
    "    mlp_classifier.zero_grad()\n",
    "    inverter.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    direction = torch.sign(c_hypo.grad)\n",
    "    nc_hypo = c_hypo + EPS * direction\n",
    "#     nhypo_idx = autoencoder.generate(nc_hypo, 10, False)\n",
    "    c_prem_full['style_hidden'] = nc_hypo\n",
    "#     print(c_prem_full)\n",
    "    \n",
    "    \n",
    "#     nhypo_idx = model.decode(nc_hypo)\n",
    "    nhypo_idx = model.decode(c_prem_full)\n",
    "#     return nhypo_idx.squeeze(0).cpu().numpy()\n",
    "    return nhypo_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_pred(pw, hw):\n",
    "    classifier1.eval()\n",
    "\n",
    "    premise_idx = torch.tensor([vocab_classifier1.get(w, 3) for w in pw]).cuda().unsqueeze(0)\n",
    "    hypothesis_idx = torch.tensor([vocab_classifier1.get(w, 3) for w in hw]).cuda().unsqueeze(0)\n",
    "\n",
    "    return F.softmax(classifier1((premise_idx, hypothesis_idx)), 1).squeeze(0).cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum(arr):\n",
    "    a = -1\n",
    "    t = 0\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] > a:\n",
    "            a = arr[i]\n",
    "            t = i\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.UNK_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Target  tensor(0)\n",
      "<sos> several people are dancing in some sand . <eos>\n",
      "<sos> the people are at the beach . <pad> <pad>\n",
      "<sos> lots lots lots lots in lots out of spring rolls !\n",
      "Old  [0.22600839 0.36014894 0.41384265]\n",
      "New  [0.12953717 0.48788851 0.3825743 ]\n",
      "Old Pred:  2\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(2)\n",
      "<sos> a red-haired girl holds a cute baby goat .\n",
      "<sos> a girl is holding a small dog . <pad>\n",
      "<sos> lots lots of live in for watch games in fountain !\n",
      "Old  [0.5548856  0.18146656 0.2636478 ]\n",
      "New  [0.18848984 0.5135597  0.29795048]\n",
      "Old Pred:  0\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(0)\n",
      "<sos> an older lady wearing a tinkerbell jacket . <eos>\n",
      "<sos> a woman is wearing a jacket . <pad> <pad>\n",
      "<sos> wide variety of take out in the corner and take out !\n",
      "Old  [0.40246424 0.26268488 0.3348509 ]\n",
      "New  [0.14847809 0.5076462  0.34387574]\n",
      "Old Pred:  0\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(0)\n",
      "<sos> two brown dogs running through water . <eos> <pad>\n",
      "<sos> the dogs ran through the water . <pad> <pad>\n",
      "<sos> lots more for in spring rolls in spring of spring rolls !\n",
      "Old  [0.21857205 0.33753058 0.4438973 ]\n",
      "New  [0.16505475 0.55722684 0.27771845]\n",
      "Old Pred:  2\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(0)\n",
      "<sos> a fluffy dog carries a stick through snow .\n",
      "<sos> a dog is trudging through the snow . <pad>\n",
      "<sos> bonus for lots of spring rolls in spring rolls in town !\n",
      "Old  [0.34007636 0.36578748 0.2941362 ]\n",
      "New  [0.17921048 0.5510948  0.26969475]\n",
      "Old Pred:  1\n",
      "New Pred:  1\n",
      "Good:  False\n",
      "--------------------------------\n",
      "Target  tensor(0)\n",
      "<sos> two girls are in a bathing suits . <eos>\n",
      "<sos> two women are in swim suits . <pad> <pad>\n",
      "<sos> lots lots lots in in out and lots of spring rolls !\n",
      "Old  [0.34928918 0.33288434 0.31782648]\n",
      "New  [0.14464186 0.46596223 0.3893959 ]\n",
      "Old Pred:  0\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(1)\n",
      "<sos> a black dog is swimming while carrying a tennis\n",
      "<sos> a dog is swimming while carrying a racket .\n",
      "<sos> more for in one spring rolls in spring rolls !\n",
      "Old  [0.4662763  0.22942346 0.3043003 ]\n",
      "New  [0.21971834 0.51745987 0.26282182]\n",
      "Old Pred:  0\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(2)\n",
      "<sos> a girl runs down a beach . <eos> <pad>\n",
      "<sos> the girl is about to fire the gun .\n",
      "<sos> bonus for lots of lots in out in out corner !\n",
      "Old  [0.11978351 0.72735834 0.15285821]\n",
      "New  [0.22707146 0.45328107 0.31964752]\n",
      "Old Pred:  1\n",
      "New Pred:  1\n",
      "Good:  False\n",
      "--------------------------------\n",
      "Target  tensor(1)\n",
      "<sos> bald man conducts an orchestra using a pencil .\n",
      "<sos> a man conducts an orchestra of 20 people .\n",
      "<sos> reasonable for in a of out you out the next !\n",
      "Old  [0.42525405 0.34733686 0.22740911]\n",
      "New  [0.166558   0.4158962  0.41754577]\n",
      "Old Pred:  0\n",
      "New Pred:  2\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(2)\n",
      "<sos> people on a sidewalk during a snowstorm . <eos>\n",
      "<sos> the people are sitting on the sandy beach .\n",
      "<sos> more of in on hold in in of spring rolls !\n",
      "Old  [0.14844714 0.1372071  0.71434575]\n",
      "New  [0.12703283 0.54508984 0.32787728]\n",
      "Old Pred:  2\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(1)\n",
      "<sos> a group of young children watching a firetruck .\n",
      "<sos> the children are dreaming of being a firefighter <pad>\n",
      "<sos> bonus for lots of spring rolls in spring of town !\n",
      "Old  [0.2297669  0.36612755 0.4041055 ]\n",
      "New  [0.15411408 0.4922145  0.3536714 ]\n",
      "Old Pred:  2\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(2)\n",
      "<sos> people are viewing advertisements in an urban area .\n",
      "<sos> nobody is viewing <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<sos> reasonable prices in on a spring of spring rolls !\n",
      "Old  [0.47630087 0.236312   0.2873871 ]\n",
      "New  [0.10460538 0.52581686 0.36957768]\n",
      "Old Pred:  0\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(2)\n",
      "<sos> two girls sit beside a river under a tree\n",
      "<sos> the boys stood next to the river . <pad>\n",
      "<sos> more of $ number in in out of spring rolls !\n",
      "Old  [0.1759948  0.43130115 0.392704  ]\n",
      "New  [0.14127074 0.55825835 0.30047086]\n",
      "Old Pred:  1\n",
      "New Pred:  1\n",
      "Good:  False\n",
      "--------------------------------\n",
      "Target  tensor(2)\n",
      "<sos> a woman examines a specimen using a microscope .\n",
      "<sos> a woman eating lunch in her office . <pad>\n",
      "<sos> more for in one of out of number box !\n",
      "Old  [0.06363563 0.17903511 0.7573292 ]\n",
      "New  [0.18780701 0.42443097 0.387762  ]\n",
      "Old Pred:  2\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(2)\n",
      "<sos> three asian men in a kitchen cooking . <eos>\n",
      "<sos> three women are cooking in a kitchen . <pad>\n",
      "<sos> lots lots in lots of options in in spring rolls !\n",
      "Old  [0.23554267 0.27497306 0.4894843 ]\n",
      "New  [0.12545432 0.5737036  0.30084214]\n",
      "Old Pred:  2\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(1)\n",
      "<sos> a man is knitting own some machinery . <eos>\n",
      "<sos> he was knitting a cover for the machine .\n",
      "<sos> wide take plus of you in the out you out !\n",
      "Old  [0.1298801  0.60241747 0.2677025 ]\n",
      "New  [0.17593399 0.44738165 0.37668443]\n",
      "Old Pred:  1\n",
      "New Pred:  1\n",
      "Good:  False\n",
      "--------------------------------\n",
      "Target  tensor(1)\n",
      "<sos> a man chops up swordfish . <eos> <pad> <pad>\n",
      "<sos> a man is cooking the fish . <pad> <pad>\n",
      "<sos> wide for take take out of town and bring out in town !\n",
      "Old  [0.22662447 0.36081317 0.4125623 ]\n",
      "New  [0.16994783 0.48498663 0.34506553]\n",
      "Old Pred:  2\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(1)\n",
      "<sos> two yellow dogs run together in green grass .\n",
      "<sos> two dogs play in the grass . <pad> <pad>\n",
      "<sos> more variety of lots in in spring and spring rolls !\n",
      "Old  [0.27931407 0.28665406 0.43403187]\n",
      "New  [0.16423053 0.5460522  0.28971726]\n",
      "Old Pred:  2\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(1)\n",
      "<sos> the scottish musicians are performing for the crowd .\n",
      "<sos> they are the opening act for a concert .\n",
      "<sos> lots more available in in spring rolls for spring rolls !\n",
      "Old  [0.07119553 0.7993848  0.12941962]\n",
      "New  [0.14800109 0.45362228 0.39837664]\n",
      "Old Pred:  1\n",
      "New Pred:  1\n",
      "Good:  False\n",
      "--------------------------------\n",
      "Target  tensor(0)\n",
      "<sos> a woman sweeps the roadway between the buildings .\n",
      "<sos> the girl is sweeping the road between buildings .\n",
      "<sos> more more in one of the spring in number number ! !\n",
      "Old  [0.22717883 0.19551545 0.5773057 ]\n",
      "New  [0.16353668 0.546262   0.29020134]\n",
      "Old Pred:  2\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(0)\n",
      "<sos> the black dog has a blue collar on .\n",
      "<sos> the animal has fur . <pad> <pad> <pad> <pad>\n",
      "<sos> more than in in the in in my my book !\n",
      "Old  [0.6311709  0.19779283 0.17103627]\n",
      "New  [0.1529853  0.57867205 0.2683427 ]\n",
      "Old Pred:  0\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(1)\n",
      "<sos> this woman has brown boots on . <eos> <pad>\n",
      "<sos> a woman is wearing her favorite shoes . <pad>\n",
      "<sos> more bonus in in the out of my new ! !\n",
      "Old  [0.17109944 0.472911   0.3559896 ]\n",
      "New  [0.12871015 0.5557756  0.31551424]\n",
      "Old Pred:  1\n",
      "New Pred:  1\n",
      "Good:  False\n",
      "--------------------------------\n",
      "Target  tensor(2)\n",
      "<sos> a young boy operates the barbecue . <eos> <pad>\n",
      "<sos> a young girl operates the barbecue . <pad> <pad>\n",
      "<sos> more for in of of of the in town !\n",
      "Old  [0.2867925  0.36757788 0.34562963]\n",
      "New  [0.1845741 0.4638102 0.3516156]\n",
      "Old Pred:  1\n",
      "New Pred:  1\n",
      "Good:  False\n",
      "--------------------------------\n",
      "Target  tensor(2)\n",
      "<sos> night game at little league <eos> <pad> <pad> <pad>\n",
      "<sos> girls play outside during the day . <pad> <pad>\n",
      "<sos> reasonable plenty in of of number and in out of spring rolls !\n",
      "Old  [0.42131105 0.35263342 0.22605556]\n",
      "New  [0.1943467  0.42885146 0.3768019 ]\n",
      "Old Pred:  0\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(2)\n",
      "<sos> a person in the air on a snowboard .\n",
      "<sos> a person is skiing . <pad> <pad> <pad> <pad>\n",
      "<sos> reasonable for take out of a seat out in out !\n",
      "Old  [0.6227914  0.18269463 0.19451389]\n",
      "New  [0.19455501 0.46600628 0.33943874]\n",
      "Old Pred:  0\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(2)\n",
      "<sos> a boy plays outside after a rain shower .\n",
      "<sos> the boy is inside . <pad> <pad> <pad> <pad>\n",
      "<sos> more for in spring of spring rolls for spring rolls !\n",
      "Old  [0.4411902  0.24285902 0.31595078]\n",
      "New  [0.22800817 0.4832584  0.28873348]\n",
      "Old Pred:  0\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(2)\n",
      "<sos> man in hard hat shoveling . <eos> <pad> <pad>\n",
      "<sos> a woman uses a <oov> . <pad> <pad> <pad>\n",
      "<sos> reasonable for of of out in you out out a day !\n",
      "Old  [0.434265   0.3160382  0.24969679]\n",
      "New  [0.15446427 0.50583375 0.339702  ]\n",
      "Old Pred:  0\n",
      "New Pred:  1\n",
      "Good:  True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Target  tensor(1)\n",
      "<sos> a plane is <oov> next to some water <eos>\n",
      "<sos> the plane is on a runway . <pad> <pad>\n",
      "<sos> wide variety take out of town and you in in out !\n",
      "Old  [0.2680588  0.26103124 0.47091   ]\n",
      "New  [0.15933205 0.49197853 0.3486895 ]\n",
      "Old Pred:  2\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(2)\n",
      "<sos> a child is holding an apple in his mouth\n",
      "<sos> a kid is watching tv . <pad> <pad> <pad>\n",
      "<sos> reasonable for take out of a seat and you out !\n",
      "Old  [0.31187543 0.21894163 0.46918294]\n",
      "New  [0.19414404 0.42774403 0.378112  ]\n",
      "Old Pred:  2\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(1)\n",
      "<sos> hang glider on beach <eos> <pad> <pad> <pad> <pad>\n",
      "<sos> glider rider on beach <pad> <pad> <pad> <pad> <pad>\n",
      "<sos> good prices for lots on on spring spring spring !\n",
      "Old  [0.50757945 0.19229233 0.3001282 ]\n",
      "New  [0.1776621  0.5130726  0.30926526]\n",
      "Old Pred:  0\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(1)\n",
      "<sos> dog running in yard . <eos> <pad> <pad> <pad>\n",
      "<sos> a dog chases a ball . <pad> <pad> <pad>\n",
      "<sos> wide take of take out of town and bring out in spring !\n",
      "Old  [0.29926127 0.32800728 0.37273148]\n",
      "New  [0.17603418 0.5423654  0.28160045]\n",
      "Old Pred:  2\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "--------------------------------\n",
      "Target  tensor(1)\n",
      "<sos> a man sitting with his small dog . <eos>\n",
      "<sos> the dog is tiny <pad> <pad> <pad> <pad> <pad>\n",
      "<sos> more lots in on on in in spring of spring !\n",
      "Old  [0.454447   0.27645713 0.26909593]\n",
      "New  [0.16983977 0.5839037  0.24625659]\n",
      "Old Pred:  0\n",
      "New Pred:  1\n",
      "Good:  True\n",
      "0.78125\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "niter = 0\n",
    "evaluate = 0\n",
    "nevaluate = 0\n",
    "idx2words = dict(map(lambda x: (x[1], x[0]), corpus_vocab.items()))\n",
    "while niter < len(testloader):\n",
    "    niter += 1\n",
    "    batch = train_iter.next()\n",
    "    for p, h, t, pw, hw, pl, hl in zip(*batch):\n",
    "        nh = perturb(criterion, p.cuda(), h.cuda(), t.cuda(), pw, hw, pl, hl)\n",
    "        print('--------------------------------')\n",
    "        print('Target ', t)\n",
    "        print(' '.join(pw))\n",
    "        print(' '.join(hw))\n",
    "#         nhw = (['<sos>'] + [idx2words[i] for i in nh])[:10]\n",
    "        nhw = '<sos> ' + ' '.join(get_sentences(model.decode(nh))[0])\n",
    "        print(nhw)\n",
    "        print('Old ', classifier_pred(pw, hw))\n",
    "        print('New ', classifier_pred(pw, nhw))\n",
    "        print('Old Pred: ', maximum(classifier_pred(pw, hw)))\n",
    "        print('New Pred: ', maximum(classifier_pred(pw, nhw)))\n",
    "        print('Good: ', maximum(classifier_pred(pw, hw)) != maximum(classifier_pred(pw, nhw)))\n",
    "        evaluate = evaluate + int(maximum(classifier_pred(pw, hw)) != maximum(classifier_pred(pw, nhw)))\n",
    "        nevaluate = nevaluate + 1\n",
    "    break\n",
    "print(evaluate / nevaluate)\n",
    "print(nevaluate)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
